{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETro9oX0g4kH"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://www.github.com/huggingface/transformers\n",
    "#!pip install git+https://github.com/huggingface/accelerate\n",
    "#!pip install bitsandbytes\n",
    "#!pip install einops\n",
    "#!pip install --upgrade torch torchvision\n",
    "#!pip install scikit-learn\n",
    "#!pip install matplotlib\n",
    "#!pip install datasets\n",
    "#!pip install Bio\n",
    "#!pip install pybedtools\n",
    "#!pip install tabulate\n",
    "#!pip install enformer-pytorch\n",
    "#!pip install einops==0.5.0\n",
    "#!pip install git+https://github.com/vchiley/triton.git@triton_pre_mlir_sm90#subdirectory=python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths & CKPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Datasets\n",
    "# hg19 fasta file\n",
    "FASTA_FILE = \"/data/Dcode/gaetano/repos/fasta_files/hg19.fa\"\n",
    "\n",
    "# training files\n",
    "path_bios = '/data/Dcode/gaetano/repos/AI4Genomic/data/enhancers/biosamples/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I320YzvVg4kF"
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7jNiPHkIg4kI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-24 14:38:49.844406: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-24 14:38:49.861540: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-24 14:38:49.861564: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-24 14:38:49.872393: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-24 14:38:51.325477: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoModel\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset\n",
    "from Bio import SeqIO\n",
    "from pybedtools import BedTool\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gpu():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "clean_gpu()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get fasta hg19 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chrom2seq(capitalize=True):\n",
    "\n",
    "    chrom2seq = {}\n",
    "    for seq in SeqIO.parse(FASTA_FILE, \"fasta\"):\n",
    "        chrom2seq[seq.description.split()[0]] = seq.seq.upper() if capitalize else seq.seq\n",
    "\n",
    "    return chrom2seq\n",
    "\n",
    "chrom2seq = get_chrom2seq()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8dpH8tOsg4kJ"
   },
   "source": [
    "## Model & Tokenizer & Datasetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BigBirdForSequenceClassification, BertForSequenceClassification, BertConfig\n",
    "\n",
    "# architecture configuration\n",
    "def get_hf_model_tokenizer(model_ckpt):\n",
    "    \n",
    "    #only for dnabert\n",
    "    if 'DNABERT' in model_ckpt:\n",
    "        config = BertConfig.from_pretrained(model_ckpt)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True, config=config) \n",
    "        model = AutoModelForSequenceClassification.from_config(config=config)\n",
    "    elif 'Geneformer' in model_ckpt:\n",
    "        tokenizer = AutoTokenizer.from_pretrained('tanoManzo/Geneformer_ft_Hepg2_1kbpHG19_DHSs_H3K27AC')\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True) \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "\n",
    "    #model = BigBirdForSequenceClassification.from_pretrained(model_ckpt, trust_remote_code=True, )\n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for Gena models\n",
    "\n",
    "#import importlib\n",
    "#gena_module_name = model.__class__.__module__\n",
    "#cls = getattr(importlib.import_module(gena_module_name), 'BertForSequenceClassification')\n",
    "#model = cls.from_pretrained(model_ckpt, num_labels=2)\n",
    "#print('\\nclassification head:', model.classifier)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HepG2 data - sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bios_sequences(bios_id, path_bios=path_bios, chrom2seq=chrom2seq):\n",
    "    pos_beds = list(BedTool(f'{path_bios}{bios_id}_positive_1kb.bed'))\n",
    "    ctrl_beds = list(BedTool(f'{path_bios}{bios_id}_control_1kb.bed'))\n",
    "\n",
    "    pos_list = []\n",
    "    ctrl_list = []\n",
    "    for chr, start, end  in pos_beds:\n",
    "        pos_list.append(str(chrom2seq[chr][int(start):int(end)]))\n",
    "\n",
    "    for chr, start, end  in ctrl_beds:\n",
    "        ctrl_list.append(str(chrom2seq[chr][int(start):int(end)]))\n",
    "\n",
    "    ctrl_list = random.sample(ctrl_list, len(pos_list))\n",
    "    seq_data = []\n",
    "    seq_data.extend(pos_list)\n",
    "    seq_data.extend(ctrl_list)\n",
    "\n",
    "    labels_data = []\n",
    "    labels_data.extend([1 for _ in range(len(pos_list))])\n",
    "    labels_data.extend([0 for _ in range(len(ctrl_list))])\n",
    "\n",
    "    return seq_data, labels_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe and remove Ns seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_sample(seq_data, labels_data, sample_size):\n",
    "    # Create DataFrame\n",
    "    bioS = pd.DataFrame({'seq_data': seq_data, 'labels': labels_data})\n",
    "\n",
    "    # Filter out rows with sequences consisting only of the same character (presumably Ns)\n",
    "    bioS_no_Ns = bioS[bioS['seq_data'].apply(lambda x: len(set(x)) > 1)]\n",
    "\n",
    "    # take a sample based on sample_size\n",
    "    bioS_no_Ns_sampled = bioS_no_Ns.sample(round(len(bioS_no_Ns)*sample_size),random_state=10)\n",
    "    bioS_no_Ns_sampled['labels'].value_counts()\n",
    "    return bioS_no_Ns_sampled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pkplSPeIg4kM"
   },
   "outputs": [],
   "source": [
    "def datasets_split_train_val_test(bioS_no_Ns_sampled):\n",
    "    # Get training data\n",
    "    train_sequences_bioS = bioS_no_Ns_sampled['seq_data'].tolist()\n",
    "    train_labels_bioS = bioS_no_Ns_sampled['labels'].tolist()\n",
    "\n",
    "    # Split the dataset into a training and a validation dataset\n",
    "    train_sequences_bioS, test_sequences_bioS, train_labels_bioS, test_labels_bioS = train_test_split(train_sequences_bioS,\n",
    "                                                                                  train_labels_bioS, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Split the test data into validation and test sets\n",
    "    validation_sequences_bioS, test_sequences_bioS, validation_labels_bioS, test_labels_bioS = train_test_split(test_sequences_bioS, test_labels_bioS, test_size=0.50, random_state=42)\n",
    "\n",
    "    # Create datasets from dictionaries\n",
    "    ds_train_bioS = Dataset.from_dict({\"data\": train_sequences_bioS, \"labels\": train_labels_bioS})\n",
    "    ds_validation_bioS = Dataset.from_dict({\"data\": validation_sequences_bioS, \"labels\": validation_labels_bioS})\n",
    "    ds_test_bioS = Dataset.from_dict({\"data\": test_sequences_bioS, \"labels\": test_labels_bioS})\n",
    "    \n",
    "    return ds_train_bioS, ds_validation_bioS, ds_test_bioS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "u26edP1Kg4kM"
   },
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8WHRcViIg4kM"
   },
   "outputs": [],
   "source": [
    "def get_tokenized_dataset(tokenizer, max_length=512):\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # Encode sequences\n",
    "        encoding = tokenizer(\n",
    "            examples['data'],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Add labels to the encoding\n",
    "        encoding['labels'] = examples['labels']\n",
    "        \n",
    "        return encoding\n",
    "    \n",
    "    # Tokenize and process the datasets\n",
    "    tokenized_train = ds_train_bioS.map(tokenize_function, batched=True, remove_columns=[\"data\"])\n",
    "    tokenized_validation = ds_validation_bioS.map(tokenize_function, batched=True, remove_columns=[\"data\"])\n",
    "    tokenized_test = ds_test_bioS.map(tokenize_function, batched=True, remove_columns=[\"data\"])\n",
    "\n",
    "    return tokenized_train, tokenized_validation, tokenized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def create_training_args(output_dir, batch_size, learning_rate, num_epochs, log_steps, token):\n",
    "    training_args = TrainingArguments(\n",
    "        hub_model_id= output_dir.split('/')[1],\n",
    "        output_dir=output_dir,  # Directory to save model and logs\n",
    "        per_device_train_batch_size=batch_size,  # Training batch size per device\n",
    "        per_device_eval_batch_size=batch_size,  # Evaluation batch size per device\n",
    "        learning_rate=learning_rate,  # Learning rate\n",
    "        num_train_epochs=num_epochs,  # Number of training epochs\n",
    "        logging_steps=log_steps,  # Logging interval\n",
    "        logging_dir='./logs',  # Directory to store logs\n",
    "        eval_strategy=\"steps\",  # Evaluation strategy\n",
    "        save_strategy=\"steps\",  # Save strategy\n",
    "        save_total_limit=3,  # Maximum number of saved models\n",
    "        disable_tqdm=False,  # Enable tqdm progress bars\n",
    "        load_best_model_at_end=True,  # Load best model at the end of training\n",
    "        metric_for_best_model=\"f1_score\",  # Metric to select the best model\n",
    "        fp16=True,  # Enable mixed precision training\n",
    "        #push_to_hub=True,  # Push model to Hugging Face hub\n",
    "        hub_token=token  # Authentication token for Hugging Face hub\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric for the evaluation using f1, auc, and prc\n",
    "def compute_metrics_classification_binary(eval_pred):\n",
    "    \"\"\"Computes F1, AUC, PRC, and other metrics for binary classification.\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    # Get probability predictions for AUC and PRC calculation (assuming it's binary classification)\n",
    "    prob_predictions = eval_pred.predictions[:, 1]  # assuming class 1 is positive\n",
    "    references = eval_pred.label_ids\n",
    "    \n",
    "    r = {\n",
    "        'f1_score': metrics.f1_score(references, predictions),\n",
    "        'precision': metrics.precision_score(references, predictions),\n",
    "        'recall': metrics.recall_score(references, predictions),\n",
    "        'accuracy': metrics.accuracy_score(references, predictions),\n",
    "        'auc': metrics.roc_auc_score(references, prob_predictions),  # AUC score\n",
    "        'prc': metrics.average_precision_score(references, prob_predictions)  # PRC (average precision score)\n",
    "    }\n",
    "    \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at AIRI-Institute/gena-lm-bert-base-t2t and are newly initialized: ['bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8473923400e841e4831e8135fbebb415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/47588 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d97564862e40459804cb737462576a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8544f0b2d2484a93928157f959c03253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5949 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='5949' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/5949 02:07 < 04:12, 15.62 it/s, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Prc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.706400</td>\n",
       "      <td>0.693553</td>\n",
       "      <td>0.688492</td>\n",
       "      <td>0.524962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.524962</td>\n",
       "      <td>0.519247</td>\n",
       "      <td>0.545541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>0.693553</td>\n",
       "      <td>0.688492</td>\n",
       "      <td>0.524962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.524962</td>\n",
       "      <td>0.519247</td>\n",
       "      <td>0.545541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.697400</td>\n",
       "      <td>0.693553</td>\n",
       "      <td>0.688492</td>\n",
       "      <td>0.524962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.524962</td>\n",
       "      <td>0.519247</td>\n",
       "      <td>0.545541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.698300</td>\n",
       "      <td>0.693553</td>\n",
       "      <td>0.688492</td>\n",
       "      <td>0.524962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.524962</td>\n",
       "      <td>0.519247</td>\n",
       "      <td>0.545541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the working device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# dataset sample size\n",
    "# 0.1 = 10%, 0.2 = 20%, .. , 1.00 = 100\n",
    "sample_size = 1.0\n",
    "\n",
    "### Model \n",
    "# model name from huggingface.co/model name_id:model_name\n",
    "#model_ckpt = 'zhihan1996/DNABERT-2-117M'\n",
    "\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-v2-50m-multi-species'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-v2-100m-multi-species'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-v2-250m-multi-species'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-v2-500m-multi-species'\n",
    "\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-500m-1000g'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-500m-human-ref'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-2.5b-1000g'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-2.5b-multi-species'\n",
    "\n",
    "#model_ckpt ='ctheodoris/Geneformer'\n",
    "\n",
    "model_ckpt = 'AIRI-Institute/gena-lm-bert-base-t2t'\n",
    "#model_ckpt = 'AIRI-Institute/gena-lm-bert-large-t2t'\n",
    "#model_ckpt = 'AIRI-Institute/gena-lm-bert-base-t2t-multi'\n",
    "#model_ckpt = 'AIRI-Institute/gena-lm-bigbird-base-sparse-t2t'\n",
    "\n",
    "\n",
    "# Define configuration parameters\n",
    "BATCH_SIZE = 8\n",
    "LOG_STEPS = 500\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 1\n",
    "TOKEN = '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'\n",
    "\n",
    "\n",
    "# samples for fine-tuning\n",
    "#'BioS2'=Hela, 'BioS45'=neural progenitor cell, 'BioS73'=hepg2, 'BioS74'=k562\n",
    "bios_ids = ['BioS2', 'BioS45', 'BioS73', 'BioS74']\n",
    "\n",
    "\n",
    "\n",
    "for bios_id in bios_ids:\n",
    "    # load model and dataset\n",
    "    model, tokenizer = get_hf_model_tokenizer(model_ckpt=model_ckpt)\n",
    "    seq_data, labels_data = get_bios_sequences(bios_id, path_bios=path_bios, chrom2seq=chrom2seq)\n",
    "    bioS_no_Ns_sampled = get_clean_sample(seq_data=seq_data, labels_data=labels_data, sample_size=sample_size)\n",
    "    ds_train_bioS, ds_validation_bioS, ds_test_bioS = datasets_split_train_val_test(bioS_no_Ns_sampled=bioS_no_Ns_sampled)\n",
    "    ds_tokenized_train, ds_tokenized_validation, ds_tokenized_test = get_tokenized_dataset(tokenizer, max_length=512)\n",
    "\n",
    "    OUTPUT_DIR = f\"ft/{model_ckpt.split('/')[1]}_ft_{bios_id}_1kbpHG19_DHSs_H3K27AC\"\n",
    "    training_args = create_training_args(OUTPUT_DIR, BATCH_SIZE, LEARNING_RATE, NUM_EPOCHS, LOG_STEPS, TOKEN)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model.to(device),\n",
    "        args=training_args,\n",
    "        train_dataset=ds_tokenized_train,\n",
    "        eval_dataset=ds_tokenized_validation,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_classification_binary,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "        )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    # push to hub\n",
    "    break\n",
    "    trainer.push_to_hub()\n",
    "    # clean gpus\n",
    "    clean_gpu()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
