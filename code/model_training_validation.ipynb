{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETro9oX0g4kH"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://www.github.com/huggingface/transformers\n",
    "#!pip install git+https://github.com/huggingface/accelerate\n",
    "#!pip install bitsandbytes\n",
    "#!pip install einops\n",
    "#!pip install --upgrade torch torchvision\n",
    "#!pip install scikit-learn\n",
    "#!pip install matplotlib\n",
    "#!pip install datasets\n",
    "#!pip install Bio\n",
    "#!pip install pybedtools\n",
    "#!pip install tabulate\n",
    "#!pip install enformer-pytorch\n",
    "#!pip install einops==0.5.0\n",
    "#!pip install git+https://github.com/vchiley/triton.git@triton_pre_mlir_sm90#subdirectory=python\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths & CKPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Datasets\n",
    "# hg19 fasta file\n",
    "FASTA_FILE = \"/data/Dcode/gaetano/repos/fasta_files/hg19.fa\"\n",
    "\n",
    "# training files\n",
    "path_bios = '/data/Dcode/gaetano/repos/AI4Genomic/data/enhancers/biosamples/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I320YzvVg4kF"
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7jNiPHkIg4kI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-29 12:15:35.125986: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-29 12:15:35.140278: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-29 12:15:35.140302: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-29 12:15:35.149955: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-29 12:15:36.384771: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoModel, BertForSequenceClassification\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset\n",
    "from Bio import SeqIO\n",
    "from pybedtools import BedTool\n",
    "from transformers import EarlyStoppingCallback\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gpu():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "clean_gpu()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get fasta hg19 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chrom2seq(capitalize=True):\n",
    "\n",
    "    chrom2seq = {}\n",
    "    for seq in SeqIO.parse(FASTA_FILE, \"fasta\"):\n",
    "        chrom2seq[seq.description.split()[0]] = seq.seq.upper() if capitalize else seq.seq\n",
    "\n",
    "    return chrom2seq\n",
    "\n",
    "chrom2seq = get_chrom2seq()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8dpH8tOsg4kJ"
   },
   "source": [
    "## Model & Tokenizer & Datasetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture configuration\n",
    "def get_hf_model_tokenizer(model_ckpt):\n",
    "    \n",
    "    if 'dnabert2' in model_ckpt:  # Only for DNABERT models\n",
    "        model_ckpt =  \"vivym/DNABERT-2-117M\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "\n",
    "\n",
    "    elif 'Geneformer' in model_ckpt:  # Geneformer model\n",
    "        tokenizer = AutoTokenizer.from_pretrained('tanoManzo/Geneformer_ft_Hepg2_1kbpHG19_DHSs_H3K27AC')\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "\n",
    "    elif 'gena-' in model_ckpt:  # Gena models\n",
    "        model = AutoModel.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "        gena_module_name = model.__class__.__module__\n",
    "        \n",
    "        if 'bigbird' in model_ckpt:  # BigBird model under Gena\n",
    "            cls = getattr(importlib.import_module(gena_module_name), 'BigBirdForSequenceClassification')\n",
    "        else:\n",
    "            cls = getattr(importlib.import_module(gena_module_name), 'BertForSequenceClassification')\n",
    "        \n",
    "        model = cls.from_pretrained(model_ckpt, num_labels=2)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "\n",
    "    else:  # Default case for other models\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HepG2 data - sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bios_sequences(bios_id, path_bios=path_bios, chrom2seq=chrom2seq):\n",
    "    pos_beds = list(BedTool(f'{path_bios}{bios_id}_positive_1kb.bed'))\n",
    "    ctrl_beds = list(BedTool(f'{path_bios}{bios_id}_control_1kb.bed'))\n",
    "\n",
    "    pos_list = []\n",
    "    ctrl_list = []\n",
    "    for chr, start, end  in pos_beds:\n",
    "        pos_list.append(str(chrom2seq[chr][int(start):int(end)]))\n",
    "\n",
    "    for chr, start, end  in ctrl_beds:\n",
    "        ctrl_list.append(str(chrom2seq[chr][int(start):int(end)]))\n",
    "\n",
    "    ctrl_list = random.sample(ctrl_list, len(pos_list))\n",
    "    seq_data = []\n",
    "    seq_data.extend(pos_list)\n",
    "    seq_data.extend(ctrl_list)\n",
    "\n",
    "    labels_data = []\n",
    "    labels_data.extend([1 for _ in range(len(pos_list))])\n",
    "    labels_data.extend([0 for _ in range(len(ctrl_list))])\n",
    "\n",
    "    return seq_data, labels_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe and remove Ns seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_sample(seq_data, labels_data, sample_size):\n",
    "    # Create DataFrame\n",
    "    bioS = pd.DataFrame({'seq_data': seq_data, 'labels': labels_data})\n",
    "\n",
    "    # Filter out rows with sequences consisting only of the same character (presumably Ns)\n",
    "    bioS_no_Ns = bioS[bioS['seq_data'].apply(lambda x: len(set(x)) > 1)]\n",
    "\n",
    "    # take a sample based on sample_size\n",
    "    bioS_no_Ns_sampled = bioS_no_Ns.sample(round(len(bioS_no_Ns)*sample_size),random_state=10)\n",
    "    bioS_no_Ns_sampled['labels'].value_counts()\n",
    "    return bioS_no_Ns_sampled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pkplSPeIg4kM"
   },
   "outputs": [],
   "source": [
    "def datasets_split_train_val_test(bioS_no_Ns_sampled):\n",
    "    # Get training data\n",
    "    train_sequences_bioS = bioS_no_Ns_sampled['seq_data'].tolist()\n",
    "    train_labels_bioS = bioS_no_Ns_sampled['labels'].tolist()\n",
    "\n",
    "    # Split the dataset into a training and a validation dataset\n",
    "    train_sequences_bioS, test_sequences_bioS, train_labels_bioS, test_labels_bioS = train_test_split(train_sequences_bioS,\n",
    "                                                                                  train_labels_bioS, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Split the test data into validation and test sets\n",
    "    validation_sequences_bioS, test_sequences_bioS, validation_labels_bioS, test_labels_bioS = train_test_split(test_sequences_bioS, test_labels_bioS, test_size=0.50, random_state=42)\n",
    "\n",
    "    # Create datasets from dictionaries\n",
    "    ds_train_bioS = Dataset.from_dict({\"data\": train_sequences_bioS, \"labels\": train_labels_bioS})\n",
    "    ds_validation_bioS = Dataset.from_dict({\"data\": validation_sequences_bioS, \"labels\": validation_labels_bioS})\n",
    "    ds_test_bioS = Dataset.from_dict({\"data\": test_sequences_bioS, \"labels\": test_labels_bioS})\n",
    "    \n",
    "    return ds_train_bioS, ds_validation_bioS, ds_test_bioS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "u26edP1Kg4kM"
   },
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8WHRcViIg4kM"
   },
   "outputs": [],
   "source": [
    "def get_tokenized_dataset(tokenizer, max_length=512):\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # Encode sequences\n",
    "        encoding = tokenizer(\n",
    "            examples['data'],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Add labels to the encoding\n",
    "        encoding['labels'] = examples['labels']\n",
    "        \n",
    "        return encoding\n",
    "    \n",
    "    # Tokenize and process the datasets\n",
    "    tokenized_train = ds_train_bioS.map(tokenize_function, batched=True, remove_columns=[\"data\"])\n",
    "    tokenized_validation = ds_validation_bioS.map(tokenize_function, batched=True, remove_columns=[\"data\"])\n",
    "    tokenized_test = ds_test_bioS.map(tokenize_function, batched=True, remove_columns=[\"data\"])\n",
    "\n",
    "    return tokenized_train, tokenized_validation, tokenized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def create_training_args(output_dir, batch_size, learning_rate, num_epochs, log_steps, token):\n",
    "    training_args = TrainingArguments(\n",
    "        hub_model_id= output_dir.split('/')[1],\n",
    "        output_dir=output_dir,  # Directory to save model and logs\n",
    "        per_device_train_batch_size=batch_size,  # Training batch size per device\n",
    "        per_device_eval_batch_size=batch_size,  # Evaluation batch size per device\n",
    "        learning_rate=learning_rate,  # Learning rate\n",
    "        num_train_epochs=num_epochs,  # Number of training epochs\n",
    "        logging_steps=log_steps,  # Logging interval\n",
    "        logging_dir='./logs',  # Directory to store logs\n",
    "        eval_strategy=\"steps\",  # Evaluation strategy\n",
    "        save_strategy=\"steps\",  # Save strategy\n",
    "        save_total_limit=3,  # Maximum number of saved models\n",
    "        disable_tqdm=False,  # Enable tqdm progress bars\n",
    "        load_best_model_at_end=True,  # Load best model at the end of training\n",
    "        metric_for_best_model=\"f1_score\",  # Metric to select the best model\n",
    "        fp16=True,  # Enable mixed precision training\n",
    "        #push_to_hub=True,  # Push model to Hugging Face hub\n",
    "        hub_token=token  # Authentication token for Hugging Face hub\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric for the evaluation using f1, auc, and prc\n",
    "def compute_metrics_classification_binary(eval_pred):\n",
    "    \"\"\"Computes F1, AUC, PRC, and other metrics for binary classification.\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    # Get probability predictions for AUC and PRC calculation (assuming it's binary classification)\n",
    "    prob_predictions = eval_pred.predictions[:, 1]  # assuming class 1 is positive\n",
    "    references = eval_pred.label_ids\n",
    "    \n",
    "    r = {\n",
    "        'f1_score': metrics.f1_score(references, predictions),\n",
    "        'precision': metrics.precision_score(references, predictions),\n",
    "        'recall': metrics.recall_score(references, predictions),\n",
    "        'accuracy': metrics.accuracy_score(references, predictions),\n",
    "        'auc': metrics.roc_auc_score(references, prob_predictions),  # AUC score\n",
    "        'prc': metrics.average_precision_score(references, prob_predictions)  # PRC (average precision score)\n",
    "    }\n",
    "    \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer class to override the _save method\n",
    "class CustomTrainer(Trainer):\n",
    "    def _save(self, output_dir, state_dict=None):\n",
    "        # Save the model with safe_serialization=False to avoid shared tensor issues\n",
    "        self.model.save_pretrained(output_dir, state_dict=state_dict, safe_serialization=False)\n",
    "        if self.tokenizer is not None:\n",
    "            self.tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at vivym/DNABERT-2-117M and are newly initialized: ['bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbc3f707b6c4278ade31439e632808c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/47492 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d959b90d5745ce87926ec41c3339fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589878a19adf4e61a461179dd033b9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5937 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Dcode/gaetano/venv/lib/python3.10/site-packages/accelerate/accelerator.py:500: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18000' max='118740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 18000/118740 16:50 < 1:34:15, 17.81 it/s, Epoch 3/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Prc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.596500</td>\n",
       "      <td>0.583609</td>\n",
       "      <td>0.766823</td>\n",
       "      <td>0.649437</td>\n",
       "      <td>0.936008</td>\n",
       "      <td>0.698838</td>\n",
       "      <td>0.787468</td>\n",
       "      <td>0.766076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.574000</td>\n",
       "      <td>0.546276</td>\n",
       "      <td>0.773140</td>\n",
       "      <td>0.697668</td>\n",
       "      <td>0.866921</td>\n",
       "      <td>0.730840</td>\n",
       "      <td>0.803018</td>\n",
       "      <td>0.787571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.559900</td>\n",
       "      <td>0.616237</td>\n",
       "      <td>0.774875</td>\n",
       "      <td>0.702381</td>\n",
       "      <td>0.864056</td>\n",
       "      <td>0.734378</td>\n",
       "      <td>0.806331</td>\n",
       "      <td>0.789331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.551600</td>\n",
       "      <td>0.543364</td>\n",
       "      <td>0.777971</td>\n",
       "      <td>0.670507</td>\n",
       "      <td>0.926457</td>\n",
       "      <td>0.720229</td>\n",
       "      <td>0.814983</td>\n",
       "      <td>0.802785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.554200</td>\n",
       "      <td>0.575944</td>\n",
       "      <td>0.642721</td>\n",
       "      <td>0.803631</td>\n",
       "      <td>0.535498</td>\n",
       "      <td>0.685026</td>\n",
       "      <td>0.815028</td>\n",
       "      <td>0.799642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.550800</td>\n",
       "      <td>0.585392</td>\n",
       "      <td>0.773570</td>\n",
       "      <td>0.649578</td>\n",
       "      <td>0.956065</td>\n",
       "      <td>0.703891</td>\n",
       "      <td>0.815331</td>\n",
       "      <td>0.804419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.543100</td>\n",
       "      <td>0.541378</td>\n",
       "      <td>0.781402</td>\n",
       "      <td>0.709535</td>\n",
       "      <td>0.869468</td>\n",
       "      <td>0.742631</td>\n",
       "      <td>0.819591</td>\n",
       "      <td>0.811347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.541600</td>\n",
       "      <td>0.559420</td>\n",
       "      <td>0.787512</td>\n",
       "      <td>0.705290</td>\n",
       "      <td>0.891436</td>\n",
       "      <td>0.745494</td>\n",
       "      <td>0.822425</td>\n",
       "      <td>0.809406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.537900</td>\n",
       "      <td>0.520911</td>\n",
       "      <td>0.787677</td>\n",
       "      <td>0.721707</td>\n",
       "      <td>0.866921</td>\n",
       "      <td>0.752737</td>\n",
       "      <td>0.827836</td>\n",
       "      <td>0.818341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.536400</td>\n",
       "      <td>0.559085</td>\n",
       "      <td>0.788534</td>\n",
       "      <td>0.705734</td>\n",
       "      <td>0.893346</td>\n",
       "      <td>0.746505</td>\n",
       "      <td>0.832339</td>\n",
       "      <td>0.821742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.541100</td>\n",
       "      <td>0.514431</td>\n",
       "      <td>0.787628</td>\n",
       "      <td>0.695440</td>\n",
       "      <td>0.907991</td>\n",
       "      <td>0.740947</td>\n",
       "      <td>0.832861</td>\n",
       "      <td>0.823956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.528000</td>\n",
       "      <td>0.588276</td>\n",
       "      <td>0.781666</td>\n",
       "      <td>0.657472</td>\n",
       "      <td>0.963706</td>\n",
       "      <td>0.715176</td>\n",
       "      <td>0.833755</td>\n",
       "      <td>0.821372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.499100</td>\n",
       "      <td>0.515535</td>\n",
       "      <td>0.794318</td>\n",
       "      <td>0.729111</td>\n",
       "      <td>0.872334</td>\n",
       "      <td>0.760990</td>\n",
       "      <td>0.838972</td>\n",
       "      <td>0.824683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>0.526386</td>\n",
       "      <td>0.791541</td>\n",
       "      <td>0.722047</td>\n",
       "      <td>0.875836</td>\n",
       "      <td>0.755937</td>\n",
       "      <td>0.819922</td>\n",
       "      <td>0.801543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.521100</td>\n",
       "      <td>0.509367</td>\n",
       "      <td>0.797332</td>\n",
       "      <td>0.696386</td>\n",
       "      <td>0.932506</td>\n",
       "      <td>0.749200</td>\n",
       "      <td>0.845358</td>\n",
       "      <td>0.837388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.493000</td>\n",
       "      <td>0.505269</td>\n",
       "      <td>0.801472</td>\n",
       "      <td>0.721345</td>\n",
       "      <td>0.901624</td>\n",
       "      <td>0.763685</td>\n",
       "      <td>0.846763</td>\n",
       "      <td>0.838718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.503700</td>\n",
       "      <td>0.501515</td>\n",
       "      <td>0.800109</td>\n",
       "      <td>0.698669</td>\n",
       "      <td>0.936008</td>\n",
       "      <td>0.752569</td>\n",
       "      <td>0.851848</td>\n",
       "      <td>0.841701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.496300</td>\n",
       "      <td>0.515424</td>\n",
       "      <td>0.793416</td>\n",
       "      <td>0.767560</td>\n",
       "      <td>0.821076</td>\n",
       "      <td>0.773791</td>\n",
       "      <td>0.848373</td>\n",
       "      <td>0.839833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.483500</td>\n",
       "      <td>0.485596</td>\n",
       "      <td>0.806219</td>\n",
       "      <td>0.724962</td>\n",
       "      <td>0.907991</td>\n",
       "      <td>0.769075</td>\n",
       "      <td>0.854478</td>\n",
       "      <td>0.848231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.492100</td>\n",
       "      <td>0.479611</td>\n",
       "      <td>0.796652</td>\n",
       "      <td>0.776201</td>\n",
       "      <td>0.818211</td>\n",
       "      <td>0.779013</td>\n",
       "      <td>0.857471</td>\n",
       "      <td>0.847500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.469700</td>\n",
       "      <td>0.489682</td>\n",
       "      <td>0.811291</td>\n",
       "      <td>0.728702</td>\n",
       "      <td>0.914995</td>\n",
       "      <td>0.774802</td>\n",
       "      <td>0.860932</td>\n",
       "      <td>0.856142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.485700</td>\n",
       "      <td>0.469412</td>\n",
       "      <td>0.812187</td>\n",
       "      <td>0.755270</td>\n",
       "      <td>0.878383</td>\n",
       "      <td>0.785077</td>\n",
       "      <td>0.861308</td>\n",
       "      <td>0.854458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.483700</td>\n",
       "      <td>0.464772</td>\n",
       "      <td>0.808472</td>\n",
       "      <td>0.775278</td>\n",
       "      <td>0.844635</td>\n",
       "      <td>0.788277</td>\n",
       "      <td>0.865363</td>\n",
       "      <td>0.859227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.443800</td>\n",
       "      <td>0.468273</td>\n",
       "      <td>0.815059</td>\n",
       "      <td>0.740442</td>\n",
       "      <td>0.906399</td>\n",
       "      <td>0.782382</td>\n",
       "      <td>0.857026</td>\n",
       "      <td>0.846634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.455500</td>\n",
       "      <td>0.458883</td>\n",
       "      <td>0.818569</td>\n",
       "      <td>0.759956</td>\n",
       "      <td>0.886979</td>\n",
       "      <td>0.791982</td>\n",
       "      <td>0.871091</td>\n",
       "      <td>0.868111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.445800</td>\n",
       "      <td>0.469839</td>\n",
       "      <td>0.817865</td>\n",
       "      <td>0.750999</td>\n",
       "      <td>0.897803</td>\n",
       "      <td>0.788445</td>\n",
       "      <td>0.870647</td>\n",
       "      <td>0.864856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.459800</td>\n",
       "      <td>0.463129</td>\n",
       "      <td>0.786992</td>\n",
       "      <td>0.804254</td>\n",
       "      <td>0.770455</td>\n",
       "      <td>0.779350</td>\n",
       "      <td>0.870705</td>\n",
       "      <td>0.865977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.460100</td>\n",
       "      <td>0.486600</td>\n",
       "      <td>0.818622</td>\n",
       "      <td>0.740407</td>\n",
       "      <td>0.915314</td>\n",
       "      <td>0.785414</td>\n",
       "      <td>0.872239</td>\n",
       "      <td>0.866168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.467500</td>\n",
       "      <td>0.467675</td>\n",
       "      <td>0.819883</td>\n",
       "      <td>0.730167</td>\n",
       "      <td>0.934734</td>\n",
       "      <td>0.782719</td>\n",
       "      <td>0.870877</td>\n",
       "      <td>0.859857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.453400</td>\n",
       "      <td>0.451236</td>\n",
       "      <td>0.815755</td>\n",
       "      <td>0.760033</td>\n",
       "      <td>0.880293</td>\n",
       "      <td>0.789624</td>\n",
       "      <td>0.869499</td>\n",
       "      <td>0.864365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.439000</td>\n",
       "      <td>0.458029</td>\n",
       "      <td>0.828125</td>\n",
       "      <td>0.758950</td>\n",
       "      <td>0.911175</td>\n",
       "      <td>0.799899</td>\n",
       "      <td>0.874763</td>\n",
       "      <td>0.869406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.447900</td>\n",
       "      <td>0.467304</td>\n",
       "      <td>0.815057</td>\n",
       "      <td>0.796837</td>\n",
       "      <td>0.834129</td>\n",
       "      <td>0.799731</td>\n",
       "      <td>0.877393</td>\n",
       "      <td>0.872170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.446800</td>\n",
       "      <td>0.457480</td>\n",
       "      <td>0.814371</td>\n",
       "      <td>0.786477</td>\n",
       "      <td>0.844317</td>\n",
       "      <td>0.796362</td>\n",
       "      <td>0.873077</td>\n",
       "      <td>0.870294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.438700</td>\n",
       "      <td>0.457646</td>\n",
       "      <td>0.817116</td>\n",
       "      <td>0.783129</td>\n",
       "      <td>0.854187</td>\n",
       "      <td>0.797709</td>\n",
       "      <td>0.877367</td>\n",
       "      <td>0.872687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.426000</td>\n",
       "      <td>0.461493</td>\n",
       "      <td>0.822917</td>\n",
       "      <td>0.772562</td>\n",
       "      <td>0.880293</td>\n",
       "      <td>0.799562</td>\n",
       "      <td>0.876777</td>\n",
       "      <td>0.872407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.425900</td>\n",
       "      <td>0.527443</td>\n",
       "      <td>0.824473</td>\n",
       "      <td>0.749609</td>\n",
       "      <td>0.915950</td>\n",
       "      <td>0.793667</td>\n",
       "      <td>0.876763</td>\n",
       "      <td>0.871322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3371582   0.3322754 ]\n",
      " [-0.02272034  0.83447266]\n",
      " [ 0.8691406  -0.68603516]\n",
      " ...\n",
      " [ 0.43115234  0.14819336]\n",
      " [-0.06069946  0.8803711 ]\n",
      " [ 0.11700439  0.640625  ]]\n",
      "[[ 0.52734375  0.17822266]\n",
      " [ 0.14074707  0.71728516]\n",
      " [ 0.9379883  -1.6523438 ]\n",
      " ...\n",
      " [ 0.79541016 -0.34033203]\n",
      " [ 0.15490723  0.69873047]\n",
      " [ 0.1940918   0.6484375 ]]\n",
      "[[ 0.7753906  -0.03094482]\n",
      " [-0.7529297   1.5546875 ]\n",
      " [ 1.6494141  -1.9101562 ]\n",
      " ...\n",
      " [-0.00801086  0.8798828 ]\n",
      " [-0.5996094   1.4316406 ]\n",
      " [-0.609375    1.4375    ]]\n",
      "[[ 0.41357422  0.6479492 ]\n",
      " [-0.17614746  1.1630859 ]\n",
      " [ 1.1630859  -1.6464844 ]\n",
      " ...\n",
      " [ 0.23400879  0.8300781 ]\n",
      " [ 0.01432037  1.0244141 ]\n",
      " [-0.06246948  1.0888672 ]]\n",
      "[[ 0.9003906  -0.17651367]\n",
      " [ 0.4284668   0.53466797]\n",
      " [ 1.2802734  -1.1005859 ]\n",
      " ...\n",
      " [ 0.84033203 -0.0869751 ]\n",
      " [ 0.75634766  0.03735352]\n",
      " [ 0.6010742   0.2993164 ]]\n",
      "[[ 0.05511475  0.8876953 ]\n",
      " [-0.39257812  1.2451172 ]\n",
      " [ 1.1845703  -1.5048828 ]\n",
      " ...\n",
      " [ 0.3154297   0.56640625]\n",
      " [-0.1772461   1.0761719 ]\n",
      " [-0.296875    1.1787109 ]]\n",
      "[[ 0.33129883  0.50341797]\n",
      " [-0.4831543   1.4140625 ]\n",
      " [ 1.6630859  -2.203125  ]\n",
      " ...\n",
      " [ 0.24230957  0.5810547 ]\n",
      " [ 0.16931152  0.64160156]\n",
      " [-0.24963379  1.1826172 ]]\n",
      "[[ 0.45214844  0.08612061]\n",
      " [-0.6176758   1.3886719 ]\n",
      " [ 1.5351562  -2.0820312 ]\n",
      " ...\n",
      " [ 0.15222168  0.46191406]\n",
      " [-0.13598633  0.80566406]\n",
      " [-0.42358398  1.1796875 ]]\n",
      "[[ 0.24047852  0.34960938]\n",
      " [-0.65527344  1.3945312 ]\n",
      " [ 1.1337891  -1.4658203 ]\n",
      " ...\n",
      " [ 0.10467529  0.5136719 ]\n",
      " [ 0.18676758  0.36694336]\n",
      " [-0.4338379   1.1523438 ]]\n",
      "[[ 0.20178223  0.16772461]\n",
      " [-0.7709961   1.6015625 ]\n",
      " [ 1.3085938  -1.9589844 ]\n",
      " ...\n",
      " [-0.6386719   1.4873047 ]\n",
      " [-0.11993408  0.69140625]\n",
      " [-0.7138672   1.5517578 ]]\n",
      "[[ 0.3161621   0.12078857]\n",
      " [-0.4807129   1.3330078 ]\n",
      " [ 0.99316406 -1.6025391 ]\n",
      " ...\n",
      " [-0.18688965  1.015625  ]\n",
      " [ 0.07574463  0.48754883]\n",
      " [-0.44458008  1.3017578 ]]\n",
      "[[ 0.03747559  0.62060547]\n",
      " [-0.87646484  1.6494141 ]\n",
      " [ 1.1982422  -1.7226562 ]\n",
      " ...\n",
      " [-0.75        1.5625    ]\n",
      " [-0.09155273  0.7451172 ]\n",
      " [-0.86865234  1.6455078 ]]\n",
      "[[ 0.68115234 -0.29174805]\n",
      " [-1.21875     1.8769531 ]\n",
      " [ 1.4052734  -1.7421875 ]\n",
      " ...\n",
      " [-0.7915039   1.5625    ]\n",
      " [ 0.26342773  0.22753906]\n",
      " [-1.1923828   1.8486328 ]]\n",
      "[[ 1.2470703  -0.77685547]\n",
      " [-0.73828125  1.3154297 ]\n",
      " [ 1.4375     -1.7294922 ]\n",
      " ...\n",
      " [-0.5205078   1.1982422 ]\n",
      " [ 0.60546875 -0.39501953]\n",
      " [-0.7207031   1.3339844 ]]\n",
      "[[ 0.67089844 -0.5913086 ]\n",
      " [-0.82421875  1.4794922 ]\n",
      " [ 1.1826172  -1.6474609 ]\n",
      " ...\n",
      " [-0.0725708   0.8574219 ]\n",
      " [ 0.07617188  0.16101074]\n",
      " [-0.74658203  1.4179688 ]]\n",
      "[[ 1.2744141  -1.1914062 ]\n",
      " [-0.9453125   1.6611328 ]\n",
      " [ 1.5234375  -1.9658203 ]\n",
      " ...\n",
      " [ 0.13549805  0.7685547 ]\n",
      " [ 0.30444336  0.13696289]\n",
      " [-0.73583984  1.4970703 ]]\n",
      "[[ 0.89404297 -0.90527344]\n",
      " [-0.53466797  1.3935547 ]\n",
      " [ 1.234375   -1.7626953 ]\n",
      " ...\n",
      " [-0.05557251  1.        ]\n",
      " [ 0.14819336  0.27539062]\n",
      " [-0.55371094  1.4433594 ]]\n",
      "[[ 1.3105469  -1.5546875 ]\n",
      " [-0.45532227  1.1445312 ]\n",
      " [ 1.3925781  -1.7871094 ]\n",
      " ...\n",
      " [ 1.2167969  -1.3066406 ]\n",
      " [ 1.0996094  -1.234375  ]\n",
      " [-0.36376953  1.0771484 ]]\n",
      "[[ 0.56591797 -0.32250977]\n",
      " [-0.7993164   1.4726562 ]\n",
      " [ 1.7666016  -2.2890625 ]\n",
      " ...\n",
      " [-0.13586426  0.8222656 ]\n",
      " [ 0.7421875  -0.6972656 ]\n",
      " [-0.8144531   1.515625  ]]\n",
      "[[ 0.8364258  -0.8676758 ]\n",
      " [-0.7182617   1.390625  ]\n",
      " [ 0.93847656 -1.1552734 ]\n",
      " ...\n",
      " [ 0.48779297  0.01555634]\n",
      " [ 0.8378906  -0.9291992 ]\n",
      " [-0.6875      1.3886719 ]]\n",
      "[[ 1.1289062  -1.2490234 ]\n",
      " [-0.6586914   1.4394531 ]\n",
      " [ 1.5595703  -2.1367188 ]\n",
      " ...\n",
      " [ 0.19592285  0.44262695]\n",
      " [ 0.90283203 -0.9238281 ]\n",
      " [-0.6616211   1.4746094 ]]\n",
      "[[ 0.55615234 -0.14941406]\n",
      " [-0.65625     1.4365234 ]\n",
      " [ 1.1162109  -1.421875  ]\n",
      " ...\n",
      " [ 0.625      -0.18554688]\n",
      " [ 0.72021484 -0.6074219 ]\n",
      " [-0.5205078   1.3398438 ]]\n",
      "[[ 0.96484375 -0.7885742 ]\n",
      " [-1.0585938   1.8164062 ]\n",
      " [ 1.6699219  -2.015625  ]\n",
      " ...\n",
      " [ 1.2568359  -1.0458984 ]\n",
      " [ 0.984375   -0.9033203 ]\n",
      " [-1.0283203   1.8212891 ]]\n",
      "[[ 0.6064453  -0.20898438]\n",
      " [-0.8701172   1.5908203 ]\n",
      " [ 1.8144531  -2.4121094 ]\n",
      " ...\n",
      " [-0.55908203  1.3554688 ]\n",
      " [ 0.93115234 -0.8457031 ]\n",
      " [-0.92871094  1.6699219 ]]\n",
      "[[ 0.7001953  -0.4177246 ]\n",
      " [-0.8417969   1.5224609 ]\n",
      " [ 1.8662109  -2.3085938 ]\n",
      " ...\n",
      " [ 0.04626465  0.65966797]\n",
      " [ 1.25       -1.4082031 ]\n",
      " [-1.0292969   1.6933594 ]]\n",
      "[[ 0.69091797 -0.45703125]\n",
      " [-0.7519531   1.3574219 ]\n",
      " [ 1.6474609  -2.0332031 ]\n",
      " ...\n",
      " [ 0.51953125 -0.09503174]\n",
      " [ 0.984375   -1.1181641 ]\n",
      " [-0.84765625  1.4492188 ]]\n",
      "[[ 0.7421875  -0.7709961 ]\n",
      " [-0.7739258   1.3945312 ]\n",
      " [ 1.7675781  -2.1289062 ]\n",
      " ...\n",
      " [ 0.8598633  -0.9160156 ]\n",
      " [ 1.1269531  -1.3066406 ]\n",
      " [-0.98876953  1.6689453 ]]\n",
      "[[ 0.8359375  -0.71191406]\n",
      " [-1.2089844   1.7119141 ]\n",
      " [ 1.6435547  -1.9238281 ]\n",
      " ...\n",
      " [ 0.625      -0.2680664 ]\n",
      " [ 1.0263672  -1.0986328 ]\n",
      " [-1.265625    1.7802734 ]]\n",
      "[[ 0.89501953 -0.64453125]\n",
      " [-0.7060547   1.0439453 ]\n",
      " [ 1.4658203  -1.6503906 ]\n",
      " ...\n",
      " [ 0.59521484 -0.14941406]\n",
      " [ 0.90185547 -0.9248047 ]\n",
      " [-0.73828125  1.1035156 ]]\n",
      "[[ 0.50927734 -0.22717285]\n",
      " [-0.95751953  1.3613281 ]\n",
      " [ 1.7783203  -2.0761719 ]\n",
      " ...\n",
      " [ 0.52783203 -0.19360352]\n",
      " [ 1.2314453  -1.4414062 ]\n",
      " [-0.8173828   1.2314453 ]]\n",
      "[[ 1.0585938  -0.84716797]\n",
      " [-0.91748047  1.3203125 ]\n",
      " [ 1.6318359  -1.8232422 ]\n",
      " ...\n",
      " [ 0.77978516 -0.3701172 ]\n",
      " [ 1.1582031  -1.1396484 ]\n",
      " [-0.8808594   1.3037109 ]]\n",
      "[[ 1.4501953  -1.4628906 ]\n",
      " [-1.2753906   1.9423828 ]\n",
      " [ 1.859375   -2.2851562 ]\n",
      " ...\n",
      " [ 0.7998047  -0.27124023]\n",
      " [ 1.3925781  -1.5673828 ]\n",
      " [-0.8901367   1.6083984 ]]\n",
      "[[ 0.78564453 -0.41308594]\n",
      " [-0.83935547  1.4296875 ]\n",
      " [ 1.6396484  -2.1347656 ]\n",
      " ...\n",
      " [ 1.0175781  -0.68359375]\n",
      " [ 1.2929688  -1.65625   ]\n",
      " [-0.52197266  1.1357422 ]]\n",
      "[[ 1.1650391 -1.2255859]\n",
      " [-1.1591797  1.6962891]\n",
      " [ 1.9179688 -2.3652344]\n",
      " ...\n",
      " [ 1.015625  -0.7714844]\n",
      " [ 1.2958984 -1.6738281]\n",
      " [-0.9291992  1.4853516]]\n",
      "[[ 1.3554688 -1.5615234]\n",
      " [-1.1201172  1.6162109]\n",
      " [ 1.9267578 -2.375    ]\n",
      " ...\n",
      " [ 1.0927734 -0.9003906]\n",
      " [ 1.0556641 -1.3144531]\n",
      " [-0.9511719  1.4501953]]\n",
      "[[ 1.6591797 -1.9482422]\n",
      " [-1.3789062  1.9033203]\n",
      " [ 2.2246094 -2.7324219]\n",
      " ...\n",
      " [ 1.5126953 -1.5419922]\n",
      " [ 1.375     -1.7578125]\n",
      " [-1.1328125  1.6484375]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c1a1f0fbbe4f3bb353a0a0780be957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d344684e4e94bdfa7c8e998efe8cf43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c612930364644c2b42bb1dc70859743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/357M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at vivym/DNABERT-2-117M and are newly initialized: ['bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba567b1a5b824273b16fa7fec59208e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/19016 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e0a11c01614842a0f3f2eae3465246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3ddecc8d0e4e3595f9ccacc6ec9e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2377 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Dcode/gaetano/venv/lib/python3.10/site-packages/accelerate/accelerator.py:500: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5500' max='47540' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5500/47540 04:20 < 33:08, 21.15 it/s, Epoch 2/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Prc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.597900</td>\n",
       "      <td>0.522528</td>\n",
       "      <td>0.725717</td>\n",
       "      <td>0.773017</td>\n",
       "      <td>0.683871</td>\n",
       "      <td>0.730332</td>\n",
       "      <td>0.824176</td>\n",
       "      <td>0.819791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.556900</td>\n",
       "      <td>0.498722</td>\n",
       "      <td>0.773791</td>\n",
       "      <td>0.749245</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.755995</td>\n",
       "      <td>0.836869</td>\n",
       "      <td>0.831941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.531100</td>\n",
       "      <td>0.601863</td>\n",
       "      <td>0.755115</td>\n",
       "      <td>0.613293</td>\n",
       "      <td>0.982258</td>\n",
       "      <td>0.667648</td>\n",
       "      <td>0.836885</td>\n",
       "      <td>0.834984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.536400</td>\n",
       "      <td>0.507271</td>\n",
       "      <td>0.770531</td>\n",
       "      <td>0.769293</td>\n",
       "      <td>0.771774</td>\n",
       "      <td>0.760202</td>\n",
       "      <td>0.842218</td>\n",
       "      <td>0.836407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.518100</td>\n",
       "      <td>0.490671</td>\n",
       "      <td>0.767701</td>\n",
       "      <td>0.757457</td>\n",
       "      <td>0.778226</td>\n",
       "      <td>0.754312</td>\n",
       "      <td>0.844023</td>\n",
       "      <td>0.837833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.521000</td>\n",
       "      <td>0.562298</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>0.717608</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>0.753891</td>\n",
       "      <td>0.842794</td>\n",
       "      <td>0.838270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.512200</td>\n",
       "      <td>0.505299</td>\n",
       "      <td>0.752534</td>\n",
       "      <td>0.789894</td>\n",
       "      <td>0.718548</td>\n",
       "      <td>0.753471</td>\n",
       "      <td>0.844189</td>\n",
       "      <td>0.838248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.509600</td>\n",
       "      <td>0.521762</td>\n",
       "      <td>0.781129</td>\n",
       "      <td>0.750371</td>\n",
       "      <td>0.814516</td>\n",
       "      <td>0.761885</td>\n",
       "      <td>0.846295</td>\n",
       "      <td>0.838861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.503100</td>\n",
       "      <td>0.494281</td>\n",
       "      <td>0.781392</td>\n",
       "      <td>0.735755</td>\n",
       "      <td>0.833065</td>\n",
       "      <td>0.756836</td>\n",
       "      <td>0.840889</td>\n",
       "      <td>0.837934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.498300</td>\n",
       "      <td>0.555903</td>\n",
       "      <td>0.779739</td>\n",
       "      <td>0.743777</td>\n",
       "      <td>0.819355</td>\n",
       "      <td>0.758519</td>\n",
       "      <td>0.838458</td>\n",
       "      <td>0.837062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.479200</td>\n",
       "      <td>0.507759</td>\n",
       "      <td>0.768923</td>\n",
       "      <td>0.763723</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.757257</td>\n",
       "      <td>0.845003</td>\n",
       "      <td>0.839064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.30908203  0.7573242 ]\n",
      " [ 1.0869141  -0.95166016]\n",
      " [ 0.5151367  -0.2355957 ]\n",
      " ...\n",
      " [-0.44482422  0.89746094]\n",
      " [ 0.93310547 -0.7524414 ]\n",
      " [ 0.11248779  0.22753906]]\n",
      "[[-0.5629883   0.8676758 ]\n",
      " [ 1.1445312  -1.1533203 ]\n",
      " [ 0.23657227 -0.08416748]\n",
      " ...\n",
      " [-0.70410156  1.0283203 ]\n",
      " [ 1.0810547  -1.078125  ]\n",
      " [-0.19140625  0.4038086 ]]\n",
      "[[-0.5029297   1.0849609 ]\n",
      " [ 1.2275391  -1.1054688 ]\n",
      " [-0.35742188  0.88671875]\n",
      " ...\n",
      " [-0.54345703  1.1298828 ]\n",
      " [ 0.578125   -0.27270508]\n",
      " [-0.2529297   0.7504883 ]]\n",
      "[[-0.52001953  1.0585938 ]\n",
      " [ 1.3935547  -1.4589844 ]\n",
      " [ 0.32250977  0.00821686]\n",
      " ...\n",
      " [-0.54785156  1.0859375 ]\n",
      " [ 1.3652344  -1.390625  ]\n",
      " [-0.31860352  0.77978516]]\n",
      "[[-0.47485352  0.97265625]\n",
      " [ 1.6396484  -1.1318359 ]\n",
      " [ 0.11773682  0.39404297]\n",
      " ...\n",
      " [-0.3774414   0.86083984]\n",
      " [ 1.5322266  -0.99072266]\n",
      " [ 0.07543945  0.4350586 ]]\n",
      "[[-1.1855469   1.5488281 ]\n",
      " [ 1.8154297  -1.4677734 ]\n",
      " [-0.46069336  0.8354492 ]\n",
      " ...\n",
      " [-1.21875     1.5615234 ]\n",
      " [ 1.8056641  -1.4482422 ]\n",
      " [-0.87109375  1.2285156 ]]\n",
      "[[-0.1821289   0.67578125]\n",
      " [ 1.671875   -1.4287109 ]\n",
      " [ 0.1586914   0.27954102]\n",
      " ...\n",
      " [-0.39697266  0.87353516]\n",
      " [ 1.6044922  -1.328125  ]\n",
      " [ 0.0345459   0.42041016]]\n",
      "[[-0.3876953   0.953125  ]\n",
      " [ 1.1259766  -1.6318359 ]\n",
      " [-0.28515625  0.7895508 ]\n",
      " ...\n",
      " [-0.37451172  0.9189453 ]\n",
      " [ 1.1386719  -1.6298828 ]\n",
      " [-0.14880371  0.6142578 ]]\n",
      "[[-0.37475586  0.9658203 ]\n",
      " [ 1.0117188  -1.3154297 ]\n",
      " [-0.17822266  0.6591797 ]\n",
      " ...\n",
      " [-0.1541748   0.63720703]\n",
      " [ 0.7788086  -0.95166016]\n",
      " [ 0.24743652  0.01039886]]\n",
      "[[-0.69921875  1.2949219 ]\n",
      " [ 1.2011719  -1.796875  ]\n",
      " [-0.44067383  0.8911133 ]\n",
      " ...\n",
      " [-0.61816406  1.1601562 ]\n",
      " [ 1.2216797  -1.8095703 ]\n",
      " [-0.38427734  0.82910156]]\n",
      "[[-0.7294922   1.0732422 ]\n",
      " [ 1.2695312  -1.5927734 ]\n",
      " [-0.40551758  0.67333984]\n",
      " ...\n",
      " [-0.53759766  0.83496094]\n",
      " [ 1.2470703  -1.5605469 ]\n",
      " [-0.38598633  0.65966797]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b79449ac8244574a3a949c2f3d53f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1fda8b57a14e24853ec21f85ced643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b051fb3e68e44e6b94e3952c363b221c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/357M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at vivym/DNABERT-2-117M and are newly initialized: ['bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472b7995119d4217a6867f34b5ebf9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21464 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8ef320b898946b094f06e3dd26fa13a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2683 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acfd69160c56405db82d06e2668eec01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2684 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Dcode/gaetano/venv/lib/python3.10/site-packages/accelerate/accelerator.py:500: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='53660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4500/53660 03:25 < 37:27, 21.87 it/s, Epoch 1/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Prc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.551100</td>\n",
       "      <td>0.557333</td>\n",
       "      <td>0.776451</td>\n",
       "      <td>0.644537</td>\n",
       "      <td>0.976257</td>\n",
       "      <td>0.699963</td>\n",
       "      <td>0.860375</td>\n",
       "      <td>0.862357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.508700</td>\n",
       "      <td>0.516541</td>\n",
       "      <td>0.789416</td>\n",
       "      <td>0.820648</td>\n",
       "      <td>0.760475</td>\n",
       "      <td>0.783451</td>\n",
       "      <td>0.863715</td>\n",
       "      <td>0.865632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.515600</td>\n",
       "      <td>0.506178</td>\n",
       "      <td>0.811680</td>\n",
       "      <td>0.765470</td>\n",
       "      <td>0.863827</td>\n",
       "      <td>0.786060</td>\n",
       "      <td>0.867527</td>\n",
       "      <td>0.868308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.512700</td>\n",
       "      <td>0.477202</td>\n",
       "      <td>0.816021</td>\n",
       "      <td>0.764491</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.789415</td>\n",
       "      <td>0.867921</td>\n",
       "      <td>0.869362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.499100</td>\n",
       "      <td>0.506254</td>\n",
       "      <td>0.810266</td>\n",
       "      <td>0.720261</td>\n",
       "      <td>0.925978</td>\n",
       "      <td>0.768543</td>\n",
       "      <td>0.871223</td>\n",
       "      <td>0.871920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.478000</td>\n",
       "      <td>0.456872</td>\n",
       "      <td>0.811273</td>\n",
       "      <td>0.742178</td>\n",
       "      <td>0.894553</td>\n",
       "      <td>0.777861</td>\n",
       "      <td>0.871327</td>\n",
       "      <td>0.872270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.480800</td>\n",
       "      <td>0.458523</td>\n",
       "      <td>0.804073</td>\n",
       "      <td>0.808616</td>\n",
       "      <td>0.799581</td>\n",
       "      <td>0.792024</td>\n",
       "      <td>0.872346</td>\n",
       "      <td>0.873808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.487300</td>\n",
       "      <td>0.467210</td>\n",
       "      <td>0.811531</td>\n",
       "      <td>0.729654</td>\n",
       "      <td>0.914106</td>\n",
       "      <td>0.773388</td>\n",
       "      <td>0.874352</td>\n",
       "      <td>0.876085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.464900</td>\n",
       "      <td>0.475901</td>\n",
       "      <td>0.809684</td>\n",
       "      <td>0.721464</td>\n",
       "      <td>0.922486</td>\n",
       "      <td>0.768543</td>\n",
       "      <td>0.875104</td>\n",
       "      <td>0.876054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.26513672  0.35375977]\n",
      " [ 0.28808594 -0.5317383 ]\n",
      " [-0.82666016  1.1621094 ]\n",
      " ...\n",
      " [-0.6777344   0.9580078 ]\n",
      " [ 0.22058105 -0.42529297]\n",
      " [ 0.06591797 -0.18457031]]\n",
      "[[ 0.60839844 -1.0566406 ]\n",
      " [ 1.0146484  -1.7470703 ]\n",
      " [-0.83447266  1.2382812 ]\n",
      " ...\n",
      " [-0.7138672   1.0820312 ]\n",
      " [ 1.0146484  -1.7490234 ]\n",
      " [ 0.99902344 -1.7441406 ]]\n",
      "[[-0.34033203  0.328125  ]\n",
      " [ 1.4960938  -1.9462891 ]\n",
      " [-1.3330078   1.6064453 ]\n",
      " ...\n",
      " [-1.1816406   1.4189453 ]\n",
      " [ 1.5009766  -1.9501953 ]\n",
      " [ 1.46875    -1.9287109 ]]\n",
      "[[-0.21789551  0.0914917 ]\n",
      " [ 1.3496094  -1.8095703 ]\n",
      " [-1.46875     1.6992188 ]\n",
      " ...\n",
      " [-1.2441406   1.4355469 ]\n",
      " [ 1.3701172  -1.8310547 ]\n",
      " [ 1.1992188  -1.6640625 ]]\n",
      "[[-0.296875   0.3959961]\n",
      " [ 1.3779297 -1.8291016]\n",
      " [-1.4160156  1.6445312]\n",
      " ...\n",
      " [-1.3212891  1.5253906]\n",
      " [ 1.4345703 -1.8818359]\n",
      " [ 1.0224609 -1.4326172]]\n",
      "[[-0.18591309  0.05059814]\n",
      " [ 1.0292969  -1.4716797 ]\n",
      " [-1.2480469   1.3925781 ]\n",
      " ...\n",
      " [-0.9785156   1.0419922 ]\n",
      " [ 1.0722656  -1.5136719 ]\n",
      " [ 0.64208984 -1.0546875 ]]\n",
      "[[ 0.21838379 -0.34472656]\n",
      " [ 0.9506836  -1.4521484 ]\n",
      " [-1.1318359   1.25      ]\n",
      " ...\n",
      " [-1.0146484   1.0615234 ]\n",
      " [ 0.97509766 -1.4638672 ]\n",
      " [ 0.765625   -1.2988281 ]]\n",
      "[[-0.18835449  0.2565918 ]\n",
      " [ 0.7548828  -1.3603516 ]\n",
      " [-1.0195312   1.3330078 ]\n",
      " ...\n",
      " [-0.8466797   1.0625    ]\n",
      " [ 0.8208008  -1.4384766 ]\n",
      " [ 0.42407227 -0.6826172 ]]\n",
      "[[-0.23156738  0.36621094]\n",
      " [ 1.2509766  -1.8310547 ]\n",
      " [-1.3320312   1.6738281 ]\n",
      " ...\n",
      " [-1.1855469   1.4902344 ]\n",
      " [ 1.3398438  -1.9101562 ]\n",
      " [ 0.89990234 -1.3779297 ]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6925c3e64054a1f99a3d9aab2ab2745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4931d628fe3452e98aebebde1c48bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa16991b72c4d5995f51ba9949e7184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/357M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at vivym/DNABERT-2-117M and are newly initialized: ['bert.embeddings.position_embeddings.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "223f54be75f94350a39247eab34ab964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55a50d0f2cca402ebe0620424025ae54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3803 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8668eb11bd84c7399c1211511073e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Dcode/gaetano/venv/lib/python3.10/site-packages/accelerate/accelerator.py:500: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19500' max='76080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19500/76080 16:24 < 47:35, 19.81 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Prc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.589100</td>\n",
       "      <td>0.538934</td>\n",
       "      <td>0.751910</td>\n",
       "      <td>0.762791</td>\n",
       "      <td>0.741336</td>\n",
       "      <td>0.743886</td>\n",
       "      <td>0.815975</td>\n",
       "      <td>0.802627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.560200</td>\n",
       "      <td>0.573582</td>\n",
       "      <td>0.713421</td>\n",
       "      <td>0.788450</td>\n",
       "      <td>0.651431</td>\n",
       "      <td>0.726006</td>\n",
       "      <td>0.819054</td>\n",
       "      <td>0.806646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.557900</td>\n",
       "      <td>0.528534</td>\n",
       "      <td>0.762330</td>\n",
       "      <td>0.752446</td>\n",
       "      <td>0.772476</td>\n",
       "      <td>0.747831</td>\n",
       "      <td>0.825248</td>\n",
       "      <td>0.815470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.524600</td>\n",
       "      <td>0.511424</td>\n",
       "      <td>0.777326</td>\n",
       "      <td>0.742009</td>\n",
       "      <td>0.816173</td>\n",
       "      <td>0.755193</td>\n",
       "      <td>0.828971</td>\n",
       "      <td>0.816726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.528600</td>\n",
       "      <td>0.533626</td>\n",
       "      <td>0.787159</td>\n",
       "      <td>0.700039</td>\n",
       "      <td>0.899046</td>\n",
       "      <td>0.745464</td>\n",
       "      <td>0.832480</td>\n",
       "      <td>0.819257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.536500</td>\n",
       "      <td>0.652343</td>\n",
       "      <td>0.703546</td>\n",
       "      <td>0.808344</td>\n",
       "      <td>0.622803</td>\n",
       "      <td>0.725217</td>\n",
       "      <td>0.816963</td>\n",
       "      <td>0.815115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.533900</td>\n",
       "      <td>0.505800</td>\n",
       "      <td>0.790400</td>\n",
       "      <td>0.725252</td>\n",
       "      <td>0.868408</td>\n",
       "      <td>0.758875</td>\n",
       "      <td>0.835950</td>\n",
       "      <td>0.824252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.528800</td>\n",
       "      <td>0.591615</td>\n",
       "      <td>0.739189</td>\n",
       "      <td>0.794457</td>\n",
       "      <td>0.691110</td>\n",
       "      <td>0.744675</td>\n",
       "      <td>0.834800</td>\n",
       "      <td>0.822297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.527900</td>\n",
       "      <td>0.518220</td>\n",
       "      <td>0.791194</td>\n",
       "      <td>0.709896</td>\n",
       "      <td>0.893521</td>\n",
       "      <td>0.753090</td>\n",
       "      <td>0.837221</td>\n",
       "      <td>0.826648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.527300</td>\n",
       "      <td>0.540489</td>\n",
       "      <td>0.776975</td>\n",
       "      <td>0.771019</td>\n",
       "      <td>0.783024</td>\n",
       "      <td>0.764659</td>\n",
       "      <td>0.838266</td>\n",
       "      <td>0.828815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.525100</td>\n",
       "      <td>0.492561</td>\n",
       "      <td>0.781112</td>\n",
       "      <td>0.765301</td>\n",
       "      <td>0.797589</td>\n",
       "      <td>0.765974</td>\n",
       "      <td>0.839999</td>\n",
       "      <td>0.828207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.512600</td>\n",
       "      <td>0.514822</td>\n",
       "      <td>0.775551</td>\n",
       "      <td>0.773613</td>\n",
       "      <td>0.777499</td>\n",
       "      <td>0.764397</td>\n",
       "      <td>0.840073</td>\n",
       "      <td>0.828839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.510200</td>\n",
       "      <td>0.489027</td>\n",
       "      <td>0.793255</td>\n",
       "      <td>0.734388</td>\n",
       "      <td>0.862381</td>\n",
       "      <td>0.764659</td>\n",
       "      <td>0.841178</td>\n",
       "      <td>0.830229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.509000</td>\n",
       "      <td>0.500714</td>\n",
       "      <td>0.783329</td>\n",
       "      <td>0.760890</td>\n",
       "      <td>0.807132</td>\n",
       "      <td>0.766237</td>\n",
       "      <td>0.841824</td>\n",
       "      <td>0.831311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.511600</td>\n",
       "      <td>0.481810</td>\n",
       "      <td>0.781483</td>\n",
       "      <td>0.774544</td>\n",
       "      <td>0.788548</td>\n",
       "      <td>0.769130</td>\n",
       "      <td>0.846619</td>\n",
       "      <td>0.836316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.506500</td>\n",
       "      <td>0.500312</td>\n",
       "      <td>0.791952</td>\n",
       "      <td>0.690041</td>\n",
       "      <td>0.929181</td>\n",
       "      <td>0.744412</td>\n",
       "      <td>0.847602</td>\n",
       "      <td>0.838421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.494800</td>\n",
       "      <td>0.496005</td>\n",
       "      <td>0.803685</td>\n",
       "      <td>0.713396</td>\n",
       "      <td>0.920141</td>\n",
       "      <td>0.764659</td>\n",
       "      <td>0.848069</td>\n",
       "      <td>0.837273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.500500</td>\n",
       "      <td>0.486583</td>\n",
       "      <td>0.767740</td>\n",
       "      <td>0.801969</td>\n",
       "      <td>0.736313</td>\n",
       "      <td>0.766763</td>\n",
       "      <td>0.852166</td>\n",
       "      <td>0.841660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.477300</td>\n",
       "      <td>0.490339</td>\n",
       "      <td>0.803956</td>\n",
       "      <td>0.714732</td>\n",
       "      <td>0.918634</td>\n",
       "      <td>0.765448</td>\n",
       "      <td>0.853317</td>\n",
       "      <td>0.842895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.481600</td>\n",
       "      <td>0.476150</td>\n",
       "      <td>0.801345</td>\n",
       "      <td>0.767956</td>\n",
       "      <td>0.837770</td>\n",
       "      <td>0.782540</td>\n",
       "      <td>0.857688</td>\n",
       "      <td>0.847554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.490700</td>\n",
       "      <td>0.512595</td>\n",
       "      <td>0.793635</td>\n",
       "      <td>0.785820</td>\n",
       "      <td>0.801607</td>\n",
       "      <td>0.781751</td>\n",
       "      <td>0.860804</td>\n",
       "      <td>0.850544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.484700</td>\n",
       "      <td>0.482616</td>\n",
       "      <td>0.766596</td>\n",
       "      <td>0.806094</td>\n",
       "      <td>0.730789</td>\n",
       "      <td>0.767026</td>\n",
       "      <td>0.857353</td>\n",
       "      <td>0.849864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.465000</td>\n",
       "      <td>0.557673</td>\n",
       "      <td>0.801274</td>\n",
       "      <td>0.694005</td>\n",
       "      <td>0.947765</td>\n",
       "      <td>0.753879</td>\n",
       "      <td>0.863998</td>\n",
       "      <td>0.854849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.461000</td>\n",
       "      <td>0.472081</td>\n",
       "      <td>0.811096</td>\n",
       "      <td>0.722070</td>\n",
       "      <td>0.925163</td>\n",
       "      <td>0.774389</td>\n",
       "      <td>0.866519</td>\n",
       "      <td>0.856267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.442200</td>\n",
       "      <td>0.477738</td>\n",
       "      <td>0.813293</td>\n",
       "      <td>0.752348</td>\n",
       "      <td>0.884982</td>\n",
       "      <td>0.787273</td>\n",
       "      <td>0.867456</td>\n",
       "      <td>0.858547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.452400</td>\n",
       "      <td>0.494742</td>\n",
       "      <td>0.797234</td>\n",
       "      <td>0.784257</td>\n",
       "      <td>0.810648</td>\n",
       "      <td>0.784118</td>\n",
       "      <td>0.865043</td>\n",
       "      <td>0.856863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.457000</td>\n",
       "      <td>0.451098</td>\n",
       "      <td>0.804380</td>\n",
       "      <td>0.780085</td>\n",
       "      <td>0.830236</td>\n",
       "      <td>0.788588</td>\n",
       "      <td>0.869044</td>\n",
       "      <td>0.859084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.455000</td>\n",
       "      <td>0.475650</td>\n",
       "      <td>0.808501</td>\n",
       "      <td>0.711450</td>\n",
       "      <td>0.936213</td>\n",
       "      <td>0.767815</td>\n",
       "      <td>0.864949</td>\n",
       "      <td>0.853949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.461100</td>\n",
       "      <td>0.482856</td>\n",
       "      <td>0.809731</td>\n",
       "      <td>0.713356</td>\n",
       "      <td>0.936213</td>\n",
       "      <td>0.769656</td>\n",
       "      <td>0.869384</td>\n",
       "      <td>0.861056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.463500</td>\n",
       "      <td>0.450583</td>\n",
       "      <td>0.813915</td>\n",
       "      <td>0.760471</td>\n",
       "      <td>0.875439</td>\n",
       "      <td>0.790429</td>\n",
       "      <td>0.868774</td>\n",
       "      <td>0.859771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.447500</td>\n",
       "      <td>0.459709</td>\n",
       "      <td>0.812048</td>\n",
       "      <td>0.780454</td>\n",
       "      <td>0.846308</td>\n",
       "      <td>0.794899</td>\n",
       "      <td>0.873895</td>\n",
       "      <td>0.865178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.433000</td>\n",
       "      <td>0.484242</td>\n",
       "      <td>0.820189</td>\n",
       "      <td>0.743768</td>\n",
       "      <td>0.914114</td>\n",
       "      <td>0.790166</td>\n",
       "      <td>0.858971</td>\n",
       "      <td>0.840485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.446000</td>\n",
       "      <td>0.467247</td>\n",
       "      <td>0.819834</td>\n",
       "      <td>0.740876</td>\n",
       "      <td>0.917629</td>\n",
       "      <td>0.788851</td>\n",
       "      <td>0.874706</td>\n",
       "      <td>0.865616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>0.472346</td>\n",
       "      <td>0.822938</td>\n",
       "      <td>0.754820</td>\n",
       "      <td>0.904571</td>\n",
       "      <td>0.796214</td>\n",
       "      <td>0.872318</td>\n",
       "      <td>0.867212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.443200</td>\n",
       "      <td>0.488018</td>\n",
       "      <td>0.814298</td>\n",
       "      <td>0.719291</td>\n",
       "      <td>0.938222</td>\n",
       "      <td>0.775966</td>\n",
       "      <td>0.872346</td>\n",
       "      <td>0.865668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.444600</td>\n",
       "      <td>0.482114</td>\n",
       "      <td>0.818588</td>\n",
       "      <td>0.737223</td>\n",
       "      <td>0.920141</td>\n",
       "      <td>0.786484</td>\n",
       "      <td>0.875644</td>\n",
       "      <td>0.868339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.435300</td>\n",
       "      <td>0.472125</td>\n",
       "      <td>0.819111</td>\n",
       "      <td>0.777527</td>\n",
       "      <td>0.865394</td>\n",
       "      <td>0.799895</td>\n",
       "      <td>0.875397</td>\n",
       "      <td>0.866728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.423800</td>\n",
       "      <td>0.488165</td>\n",
       "      <td>0.801720</td>\n",
       "      <td>0.807438</td>\n",
       "      <td>0.796082</td>\n",
       "      <td>0.793847</td>\n",
       "      <td>0.874427</td>\n",
       "      <td>0.866318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.420700</td>\n",
       "      <td>0.450474</td>\n",
       "      <td>0.822083</td>\n",
       "      <td>0.763793</td>\n",
       "      <td>0.890005</td>\n",
       "      <td>0.798317</td>\n",
       "      <td>0.867589</td>\n",
       "      <td>0.846844</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.5878906  -1.4111328 ]\n",
      " [ 0.22290039  0.24694824]\n",
      " [ 0.17712402  0.28930664]\n",
      " ...\n",
      " [ 0.81689453 -0.43408203]\n",
      " [ 0.37963867  0.08026123]\n",
      " [ 0.6225586  -0.19995117]]\n",
      "[[ 1.8457031  -1.9941406 ]\n",
      " [ 0.09295654 -0.0690918 ]\n",
      " [ 0.3955078  -0.40722656]\n",
      " ...\n",
      " [ 0.55566406 -0.59033203]\n",
      " [ 0.2512207  -0.23742676]\n",
      " [ 0.58496094 -0.6269531 ]]\n",
      "[[ 1.6523438  -1.8466797 ]\n",
      " [-0.28271484  0.10516357]\n",
      " [ 0.02035522 -0.25463867]\n",
      " ...\n",
      " [ 0.14501953 -0.41333008]\n",
      " [-0.23400879  0.04528809]\n",
      " [ 0.31591797 -0.6010742 ]]\n",
      "[[ 1.1074219  -1.3925781 ]\n",
      " [-0.2763672   0.2783203 ]\n",
      " [ 0.05789185 -0.1373291 ]\n",
      " ...\n",
      " [ 0.12091064 -0.2253418 ]\n",
      " [-0.23535156  0.22875977]\n",
      " [ 0.07977295 -0.17004395]]\n",
      "[[ 0.94433594 -1.1269531 ]\n",
      " [-0.45996094  0.86035156]\n",
      " [-0.10754395  0.41455078]\n",
      " ...\n",
      " [-0.13781738  0.4482422 ]\n",
      " [-0.32348633  0.6899414 ]\n",
      " [-0.11309814  0.4177246 ]]\n",
      "[[ 1.5732422  -1.8398438 ]\n",
      " [ 0.23376465 -0.45263672]\n",
      " [ 1.0185547  -1.3896484 ]\n",
      " ...\n",
      " [ 1.0107422  -1.4003906 ]\n",
      " [ 0.453125   -0.74121094]\n",
      " [ 0.8808594  -1.2480469 ]]\n",
      "[[ 1.2041016  -1.5214844 ]\n",
      " [-0.3713379   0.53222656]\n",
      " [-0.21386719  0.29125977]\n",
      " ...\n",
      " [ 0.02018738 -0.06103516]\n",
      " [-0.11077881  0.12878418]\n",
      " [-0.02818298  0.00928497]]\n",
      "[[ 1.9804688  -2.1855469 ]\n",
      " [ 0.03198242 -0.05062866]\n",
      " [ 0.5830078  -0.7163086 ]\n",
      " ...\n",
      " [ 0.89404297 -1.0751953 ]\n",
      " [ 1.0947266  -1.296875  ]\n",
      " [ 1.0722656  -1.2832031 ]]\n",
      "[[ 0.84521484 -1.1230469 ]\n",
      " [-0.13085938  0.30737305]\n",
      " [-0.0383606   0.10394287]\n",
      " ...\n",
      " [-0.0072937   0.04470825]\n",
      " [-0.08569336  0.20349121]\n",
      " [ 0.02365112 -0.01097107]]\n",
      "[[ 1.5839844  -1.5927734 ]\n",
      " [-0.32080078  0.2607422 ]\n",
      " [-0.00965881 -0.06173706]\n",
      " ...\n",
      " [ 0.8208008  -0.94091797]\n",
      " [ 0.21264648 -0.32958984]\n",
      " [ 1.1650391  -1.2539062 ]]\n",
      "[[ 1.1005859  -1.4648438 ]\n",
      " [-0.46972656 -0.07897949]\n",
      " [-0.41552734 -0.14904785]\n",
      " ...\n",
      " [ 0.07904053 -0.71728516]\n",
      " [-0.17224121 -0.44995117]\n",
      " [ 0.21716309 -0.84033203]]\n",
      "[[ 1.5527344  -1.8886719 ]\n",
      " [-0.5859375   0.12158203]\n",
      " [-0.31176758 -0.1953125 ]\n",
      " ...\n",
      " [ 0.60546875 -1.1396484 ]\n",
      " [ 0.13476562 -0.6791992 ]\n",
      " [ 0.36376953 -0.9057617 ]]\n",
      "[[ 1.2900391  -1.5712891 ]\n",
      " [-0.58251953  0.1619873 ]\n",
      " [-0.5         0.05215454]\n",
      " ...\n",
      " [-0.04983521 -0.47387695]\n",
      " [-0.050354   -0.46875   ]\n",
      " [-0.16662598 -0.34643555]]\n",
      "[[ 1.4707031  -1.6142578 ]\n",
      " [-0.6669922   0.14086914]\n",
      " [-0.6352539   0.10906982]\n",
      " ...\n",
      " [ 0.27416992 -0.82421875]\n",
      " [ 0.7680664  -1.234375  ]\n",
      " [ 0.75439453 -1.2177734 ]]\n",
      "[[ 1.4365234  -1.71875   ]\n",
      " [-0.56933594  0.07330322]\n",
      " [-0.5786133   0.08074951]\n",
      " ...\n",
      " [ 0.1706543  -0.71191406]\n",
      " [ 0.31079102 -0.8388672 ]\n",
      " [ 0.4489746  -0.9628906 ]]\n",
      "[[ 1.6064453  -1.6386719 ]\n",
      " [-0.5576172   0.26635742]\n",
      " [-0.6044922   0.3161621 ]\n",
      " ...\n",
      " [ 0.04907227 -0.41503906]\n",
      " [-0.07653809 -0.2770996 ]\n",
      " [ 0.0402832  -0.40234375]]\n",
      "[[ 1.5458984  -1.6132812 ]\n",
      " [-0.56347656  0.6464844 ]\n",
      " [-0.58447266  0.65722656]\n",
      " ...\n",
      " [-0.12927246  0.06896973]\n",
      " [-0.20678711  0.16687012]\n",
      " [ 0.6464844  -0.7890625 ]]\n",
      "[[ 1.1738281  -1.21875   ]\n",
      " [-0.31811523  0.43237305]\n",
      " [-0.54296875  0.64453125]\n",
      " ...\n",
      " [ 0.7294922  -0.74853516]\n",
      " [ 0.44970703 -0.4345703 ]\n",
      " [ 0.5161133  -0.51464844]]\n",
      "[[ 0.9633789  -0.9604492 ]\n",
      " [-0.42822266  0.6098633 ]\n",
      " [-0.47143555  0.62939453]\n",
      " ...\n",
      " [ 0.36108398 -0.26367188]\n",
      " [-0.25463867  0.42089844]\n",
      " [ 0.06817627  0.06207275]]\n",
      "[[ 1.7929688  -1.8847656 ]\n",
      " [-0.6669922   0.6352539 ]\n",
      " [-0.671875    0.60498047]\n",
      " ...\n",
      " [ 0.8330078  -0.94970703]\n",
      " [-0.11297607  0.07006836]\n",
      " [ 0.81933594 -0.93066406]]\n",
      "[[ 1.90625    -1.9042969 ]\n",
      " [-0.97265625  0.6435547 ]\n",
      " [-1.1396484   0.7792969 ]\n",
      " ...\n",
      " [ 1.109375   -1.3759766 ]\n",
      " [-0.25952148 -0.01113892]\n",
      " [ 0.87060547 -1.125     ]]\n",
      "[[ 1.3759766  -1.5449219 ]\n",
      " [-0.42041016  0.18823242]\n",
      " [-0.9604492   0.71240234]\n",
      " ...\n",
      " [ 0.6557617  -0.9458008 ]\n",
      " [ 0.48461914 -0.76708984]\n",
      " [ 0.6665039  -0.9560547 ]]\n",
      "[[ 1.9042969  -1.9550781 ]\n",
      " [-1.3457031   1.1484375 ]\n",
      " [-1.5498047   1.3583984 ]\n",
      " ...\n",
      " [-0.09210205 -0.10174561]\n",
      " [-0.71435547  0.5126953 ]\n",
      " [-0.13793945 -0.06463623]]\n",
      "[[ 1.5068359  -1.5986328 ]\n",
      " [-0.91552734  0.7138672 ]\n",
      " [-1.3447266   1.1308594 ]\n",
      " ...\n",
      " [ 0.01386261 -0.17675781]\n",
      " [-0.32299805  0.1595459 ]\n",
      " [ 0.00641632 -0.1673584 ]]\n",
      "[[ 1.7783203  -1.8115234 ]\n",
      " [-1.1572266   0.9428711 ]\n",
      " [-1.5292969   1.3154297 ]\n",
      " ...\n",
      " [ 0.28759766 -0.41015625]\n",
      " [-0.23864746  0.1114502 ]\n",
      " [ 0.29223633 -0.41308594]]\n",
      "[[ 2.2226562  -2.1230469 ]\n",
      " [-1.0859375   0.81347656]\n",
      " [-1.2138672   0.88720703]\n",
      " ...\n",
      " [ 1.0761719  -1.1943359 ]\n",
      " [ 0.703125   -0.8046875 ]\n",
      " [ 0.80078125 -0.90478516]]\n",
      "[[ 2.0605469  -1.9160156 ]\n",
      " [-0.7446289   0.6430664 ]\n",
      " [-0.89160156  0.7397461 ]\n",
      " ...\n",
      " [ 0.40771484 -0.3959961 ]\n",
      " [ 0.4831543  -0.46411133]\n",
      " [ 0.54345703 -0.53808594]]\n",
      "[[ 1.4716797  -1.5214844 ]\n",
      " [-0.5854492   0.50146484]\n",
      " [-0.8823242   0.7553711 ]\n",
      " ...\n",
      " [ 0.07122803 -0.10699463]\n",
      " [-0.08996582  0.05334473]\n",
      " [-0.04684448  0.01126862]]\n",
      "[[ 1.3808594  -1.4990234 ]\n",
      " [-1.0292969   0.86328125]\n",
      " [-1.1835938   1.0126953 ]\n",
      " ...\n",
      " [ 0.1315918  -0.3474121 ]\n",
      " [-0.12469482 -0.06872559]\n",
      " [-0.06204224 -0.13769531]]\n",
      "[[ 1.796875   -1.84375   ]\n",
      " [-0.9296875   0.79541016]\n",
      " [-1.1337891   1.0136719 ]\n",
      " ...\n",
      " [ 0.5644531  -0.8144531 ]\n",
      " [ 0.17126465 -0.37841797]\n",
      " [ 0.31811523 -0.54003906]]\n",
      "[[ 1.6337891  -1.5361328 ]\n",
      " [-1.1181641   0.9375    ]\n",
      " [-1.1630859   0.96533203]\n",
      " ...\n",
      " [ 0.8408203  -0.95410156]\n",
      " [ 0.57666016 -0.70458984]\n",
      " [ 0.69433594 -0.8208008 ]]\n",
      "[[ 2.0058594  -1.8886719 ]\n",
      " [-1.1240234   1.0673828 ]\n",
      " [-1.1904297   1.1328125 ]\n",
      " ...\n",
      " [ 0.33398438 -0.40307617]\n",
      " [ 0.55859375 -0.62939453]\n",
      " [ 0.49389648 -0.5654297 ]]\n",
      "[[ 1.8427734  -1.8310547 ]\n",
      " [-1.0585938   1.0263672 ]\n",
      " [-1.0390625   0.97998047]\n",
      " ...\n",
      " [-0.03009033  0.00510788]\n",
      " [ 0.4099121  -0.44360352]\n",
      " [ 0.31933594 -0.3503418 ]]\n",
      "[[ 2.0917969  -1.9667969 ]\n",
      " [-1.1992188   1.1933594 ]\n",
      " [-1.4179688   1.3984375 ]\n",
      " ...\n",
      " [ 0.19909668 -0.19970703]\n",
      " [ 0.46362305 -0.4645996 ]\n",
      " [ 0.49902344 -0.5048828 ]]\n",
      "[[ 1.7607422  -1.6835938 ]\n",
      " [-1.0224609   0.94384766]\n",
      " [-1.1630859   1.0849609 ]\n",
      " ...\n",
      " [-0.44873047  0.29736328]\n",
      " [ 0.0836792  -0.22131348]\n",
      " [-0.24511719  0.09686279]]\n",
      "[[ 2.2363281  -2.0839844 ]\n",
      " [-1.1855469   1.1191406 ]\n",
      " [-1.2119141   1.1494141 ]\n",
      " ...\n",
      " [-0.21081543  0.10992432]\n",
      " [ 0.10888672 -0.18786621]\n",
      " [ 0.22265625 -0.30249023]]\n",
      "[[ 1.7753906e+00 -1.8603516e+00]\n",
      " [-1.1679688e+00  1.1416016e+00]\n",
      " [-1.5927734e+00  1.6044922e+00]\n",
      " ...\n",
      " [ 3.6694336e-01 -5.2001953e-01]\n",
      " [ 8.7070465e-04 -1.2573242e-01]\n",
      " [ 8.3056641e-01 -9.9853516e-01]]\n",
      "[[ 2.1933594 -2.1542969]\n",
      " [-1.1142578  1.1035156]\n",
      " [-1.4296875  1.4189453]\n",
      " ...\n",
      " [ 1.1669922 -1.3291016]\n",
      " [ 1.1347656 -1.2949219]\n",
      " [ 1.1279297 -1.2802734]]\n",
      "[[ 2.4335938e+00 -2.2636719e+00]\n",
      " [-1.2109375e+00  1.0791016e+00]\n",
      " [-1.7509766e+00  1.7431641e+00]\n",
      " ...\n",
      " [ 3.1909180e-01 -5.3369141e-01]\n",
      " [ 1.0595703e-01 -3.1738281e-01]\n",
      " [-5.0067902e-05 -2.1118164e-01]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfeb235828f401fbd50183d2b5258e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3085405614744bfb59e2fde542f7259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/5.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa3c72063eeb4d649d0194434285384c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/357M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the working device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# dataset sample size\n",
    "# 0.1 = 10%, 0.2 = 20%, .. , 1.00 = 100\n",
    "sample_size = 1.0\n",
    "\n",
    "### Model \n",
    "# model name from huggingface.co/model name_id:model_name\n",
    "model_ckpt = 'czl/dnabert2'\n",
    "\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-v2-50m-multi-species'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-v2-100m-multi-species'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-v2-250m-multi-species'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-v2-500m-multi-species'\n",
    "\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-500m-1000g'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-500m-human-ref'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-2.5b-1000g'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-2.5b-multi-species'\n",
    "\n",
    "#model_ckpt ='ctheodoris/Geneformer'\n",
    "\n",
    "#model_ckpt = 'AIRI-Institute/gena-lm-bert-base-t2t'\n",
    "#model_ckpt = 'AIRI-Institute/gena-lm-bert-large-t2t'\n",
    "#model_ckpt = 'AIRI-Institute/gena-lm-bert-base-t2t-multi'\n",
    "#model_ckpt = 'AIRI-Institute/gena-lm-bigbird-base-t2t'\n",
    "\n",
    "#model_ckpt = 'LongSafari/hyenadna-small-32k-seqlen-hf'\n",
    "#model_ckpt = 'LongSafari/hyenadna-medium-160k-seqlen-hf'\n",
    "#model_ckpt = 'LongSafari/hyenadna-medium-450k-seqlen-hf'\n",
    "#model_ckpt = 'LongSafari/hyenadna-large-1m-seqlen-hf'\n",
    "\n",
    "\n",
    "# Define configuration parameters\n",
    "BATCH_SIZE = 8\n",
    "LOG_STEPS = 500\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 20\n",
    "TOKEN = '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'\n",
    "\n",
    "\n",
    "# samples for fine-tuning\n",
    "#'BioS2'=Hela, 'BioS45'=neural progenitor cell, 'BioS73'=hepg2, 'BioS74'=k562\n",
    "bios_ids = ['BioS2', 'BioS45', 'BioS73', 'BioS74']\n",
    "\n",
    "\n",
    "\n",
    "for bios_id in bios_ids:\n",
    "    # load model and dataset\n",
    "    model, tokenizer = get_hf_model_tokenizer(model_ckpt=model_ckpt)\n",
    "    seq_data, labels_data = get_bios_sequences(bios_id, path_bios=path_bios, chrom2seq=chrom2seq)\n",
    "    bioS_no_Ns_sampled = get_clean_sample(seq_data=seq_data, labels_data=labels_data, sample_size=sample_size)\n",
    "    ds_train_bioS, ds_validation_bioS, ds_test_bioS = datasets_split_train_val_test(bioS_no_Ns_sampled=bioS_no_Ns_sampled)\n",
    "    ds_tokenized_train, ds_tokenized_validation, ds_tokenized_test = get_tokenized_dataset(tokenizer, max_length=512)\n",
    "    model.config.use_flash_attention = False  \n",
    "    OUTPUT_DIR = f\"ft/{model_ckpt.split('/')[1]}_ft_{bios_id}_1kbpHG19_DHSs_H3K27AC\"\n",
    "    training_args = create_training_args(OUTPUT_DIR, BATCH_SIZE, LEARNING_RATE, NUM_EPOCHS, LOG_STEPS, TOKEN)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model.to(device),\n",
    "        args=training_args,\n",
    "        train_dataset=ds_tokenized_train,\n",
    "        eval_dataset=ds_tokenized_validation,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_classification_binary,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    # push to hub\n",
    "    trainer.push_to_hub()\n",
    "    # clean gpus\n",
    "    clean_gpu()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7211914   0.7060547 ]\n",
      " [ 2.0644531  -1.9433594 ]\n",
      " [-1.4130859   1.3886719 ]\n",
      " ...\n",
      " [-0.9267578   0.88720703]\n",
      " [ 0.17858887 -0.16992188]\n",
      " [-1.2646484   1.2451172 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.7211914 ,  0.7060547 ],\n",
       "       [ 2.0644531 , -1.9433594 ],\n",
       "       [-1.4130859 ,  1.3886719 ],\n",
       "       ...,\n",
       "       [-0.9267578 ,  0.88720703],\n",
       "       [ 0.17858887, -0.16992188],\n",
       "       [-1.2646484 ,  1.2451172 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 0, 1, 1]), metrics={'test_loss': 0.475721150636673, 'test_f1_score': 0.8197463768115942, 'test_precision': 0.7494824016563147, 'test_recall': 0.9045477261369316, 'test_accuracy': 0.7907465825446898, 'test_auc': 0.868190003722487, 'test_prc': 0.8528215378984432, 'test_runtime': 5.09, 'test_samples_per_second': 747.35, 'test_steps_per_second': 93.517})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(ds_tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7211914   0.7060547 ]\n",
      " [ 2.0644531  -1.9433594 ]\n",
      " [-1.4130859   1.3886719 ]\n",
      " ...\n",
      " [-0.9267578   0.88720703]\n",
      " [ 0.17858887 -0.16992188]\n",
      " [-1.2646484   1.2451172 ]]\n",
      "Metric                        Value\n",
      "-----------------------  ----------\n",
      "test_loss                  0.475721\n",
      "test_f1_score              0.819746\n",
      "test_precision             0.749482\n",
      "test_recall                0.904548\n",
      "test_accuracy              0.790747\n",
      "test_auc                   0.86819\n",
      "test_prc                   0.852822\n",
      "test_runtime               5.0971\n",
      "test_samples_per_second  746.309\n",
      "test_steps_per_second     93.387\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.predict(ds_tokenized_test)\n",
    "\n",
    "\n",
    "# Print evaluation results in a table format\n",
    "print(tabulate(eval_results.metrics.items(), headers=[\"Metric\", \"Value\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(eval_results.predictions, axis=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
