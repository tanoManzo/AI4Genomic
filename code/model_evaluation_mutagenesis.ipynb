{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETro9oX0g4kH"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install git+https://www.github.com/huggingface/transformers\n",
        "#!pip install git+https://github.com/huggingface/accelerate\n",
        "#!pip install bitsandbytes\n",
        "#!pip install einops\n",
        "#!pip install --upgrade torch torchvision\n",
        "#!pip install scikit-learn\n",
        "#!pip install matplotlib\n",
        "#!pip install datasets\n",
        "#!pip install Bio\n",
        "#!pip install pybedtools\n",
        "#!pip install tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I320YzvVg4kF"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7jNiPHkIg4kI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-02 16:49:28.470287: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-10-02 16:49:28.470317: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-10-02 16:49:28.471065: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-10-02 16:49:28.476054: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-10-02 16:49:29.482106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification,BertForSequenceClassification, AutoModel, AutoConfig\n",
        "from transformers.models.bert.configuration_bert import BertConfig\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "from sklearn import metrics \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from Bio import SeqIO\n",
        "from pybedtools import BedTool\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import importlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_HOME'] = './cache/'\n",
        "os.environ[\"TRANSFORMERS_CACHE\"] = './cache/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Transformer Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_models_and_tokenizers(models_names, bios_id, ft_model_type):\n",
        "    models_tokenizers_dict = {}\n",
        "\n",
        "    for model_name in models_names:\n",
        "        model_ckpt = f\"tanoManzo/{model_name}_ft_{bios_id}_{ft_model_type}\"\n",
        "        print(f\"Loading model and tokenizer for: {model_ckpt}\")\n",
        "\n",
        "        try:\n",
        "            # Load DNABERT model\n",
        "            if 'dnabert2' in model_ckpt:\n",
        "               model = BertForSequenceClassification.from_pretrained(model_ckpt, trust_remote_code=True)\n",
        "               tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "            \n",
        "            # Load Geneformer model\n",
        "            elif 'Geneformer' in model_ckpt:\n",
        "                tokenizer = AutoTokenizer.from_pretrained('tanoManzo/Geneformer_ft_Hepg2_1kbpHG19_DHSs_H3K27AC')\n",
        "                model = AutoModelForSequenceClassification.from_pretrained(model_ckpt)\n",
        "\n",
        "            # Load Gena models\n",
        "            elif 'gena-' in model_ckpt:\n",
        "                model = AutoModel.from_pretrained(model_ckpt, trust_remote_code=True)\n",
        "                gena_module_name = model.__class__.__module__\n",
        "\n",
        "                # BigBird model under Gena\n",
        "                if 'bigbird' in model_ckpt:\n",
        "                    cls = getattr(importlib.import_module(gena_module_name), 'BigBirdForSequenceClassification')\n",
        "                else:\n",
        "                    cls = getattr(importlib.import_module(gena_module_name), 'BertForSequenceClassification')\n",
        "                \n",
        "                model = cls.from_pretrained(model_ckpt, num_labels=2)\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True)\n",
        "\n",
        "            # Load generic model\n",
        "            else:\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True)\n",
        "                model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, trust_remote_code=True)\n",
        "\n",
        "            # Store the model and tokenizer in a dictionary\n",
        "            models_tokenizers_dict[f\"{model_name}_ft_{bios_id}\"] = {'model': model, 'tokenizer': tokenizer}\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {model_ckpt}: {str(e)}\")\n",
        "\n",
        "    return models_tokenizers_dict\n",
        "\n",
        "# Example usage\n",
        "#models_tokenizers_dict = load_models_and_tokenizers(models_names, bios_id, ft_model_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq84tE1Zg4kJ"
      },
      "source": [
        "## Datasetes"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### get fasta hg19/hg38 database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_chrom2seq(fasta_file, capitalize=True):\n",
        "\n",
        "    chrom2seq = {}\n",
        "    for seq in SeqIO.parse(fasta_file, \"fasta\"):\n",
        "        chrom2seq[seq.description.split()[0]] = seq.seq.upper() if capitalize else seq.seq\n",
        "\n",
        "    return chrom2seq\n",
        "# Example usage\n",
        "#chrom2seq = get_chrom2seq(FASTA_FILE_19)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_preprocessing(type_data,name_data,dataset_path):\n",
        "    \n",
        "    updated_data_df = pd.DataFrame()\n",
        "    path_file = f\"{dataset_path}{name_data}\"\n",
        "\n",
        "    if type_data == 'raQTL':\n",
        "        old_data_df = pd.read_csv(path_file, sep='\\t')\n",
        "        updated_data_df['Chromosome'] = old_data_df['chr']\n",
        "        updated_data_df['Position'] = old_data_df['SNPabspos']\n",
        "        updated_data_df['Reference'] = old_data_df['ref']\n",
        "        updated_data_df['Alternative'] = old_data_df['alt']\n",
        "        if 'hepg2' in  name_data:\n",
        "            type_cell = 'hepg2'\n",
        "        else:\n",
        "            type_cell = 'k562'    \n",
        "        updated_data_df['Value_Ratio'] = old_data_df[f'{type_cell}.alt.mean']/old_data_df[f'{type_cell}.ref.mean']\n",
        "        updated_data_df['Value_Diff'] = old_data_df[f'{type_cell}.alt.mean']-old_data_df[f'{type_cell}.ref.mean']\n",
        "        updated_data_df['Value_Pvalue_signed'] = -np.log10(old_data_df[f'{type_cell}.wilcox.p.value'])*np.sign(updated_data_df['Value_Diff'])\n",
        "        updated_data_df['P_value'] = old_data_df[f'{type_cell}.wilcox.p.value']\n",
        "    elif type_data == 'mpra':\n",
        "        old_data_df = pd.read_csv(path_file)\n",
        "        if 'GSE87711' in name_data:  \n",
        "            updated_data_df['Chromosome'] = old_data_df['chr'].apply(lambda x: f'chr{x}')\n",
        "            updated_data_df['Position'] = old_data_df['pos']\n",
        "            updated_data_df['Reference'] = old_data_df['ref']\n",
        "            updated_data_df['Alternative'] = old_data_df['alt']\n",
        "            updated_data_df['Value_Ratio'] = old_data_df['CTRL.fc(log2)']\n",
        "            updated_data_df['Value_Diff'] = old_data_df['CTRL.padj']-old_data_df['CTRL.mut.padj']\n",
        "            updated_data_df['Value_Pvalue_signed'] = -np.log10(old_data_df['CTRL.mut.p'])*np.sign(updated_data_df['Value_Diff'])\n",
        "            updated_data_df['P_value'] = old_data_df['CTRL.mut.p']\n",
        "        if 'SORT1' in name_data:\n",
        "            updated_data_df['Chromosome'] = old_data_df['Chromosome'].apply(lambda x: f'chr{x}')\n",
        "            updated_data_df['Position'] = old_data_df['Position']\n",
        "            updated_data_df['Reference'] = old_data_df['Ref']\n",
        "            updated_data_df['Alternative'] = old_data_df['Alt']\n",
        "            updated_data_df['Value_Ratio'] = old_data_df['VariantExpressionEffect (log2)']\n",
        "            updated_data_df['Value_Pvalue_signed'] = -np.log10(old_data_df['P-value'])*np.sign(updated_data_df['Value_Ratio'])\n",
        "            updated_data_df['P_value'] = old_data_df['P-value']\n",
        "        if 'GSE68331' in name_data:\n",
        "            updated_data_df['Chromosome'] = old_data_df['chr3']\n",
        "            updated_data_df['Position'] = old_data_df['Pos']\n",
        "            updated_data_df['Reference'] = old_data_df['Allele0']\n",
        "            updated_data_df['Alternative'] = old_data_df['Allele1']\n",
        "            updated_data_df['Value_Ratio'] = old_data_df['effect']\n",
        "            updated_data_df['Value_Pvalue_signed'] = -np.log10(old_data_df['P'])*np.sign(np.log2(old_data_df['effect']))\n",
        "            updated_data_df['P_value'] = old_data_df['P']\n",
        "            \n",
        "    return updated_data_df\n",
        "\n",
        "# Example usage\n",
        "#data_df = data_preprocessing(type_data,name_data,dataset_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract Sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_sequences(data_df, chrom2seq, length_bp=999):\n",
        "    \"\"\"\n",
        "    Process sequences from a DataFrame and extract reference and alternative sequences.\n",
        "\n",
        "    Parameters:\n",
        "        mpra_df (pd.DataFrame): DataFrame containing chromosome, position, alt, and p-value columns.\n",
        "        chrom2seq (dict): Dictionary mapping chromosomes to sequence data.\n",
        "        length_bp (int): Length of the sequence to extract centered around each position.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing three lists:\n",
        "            - seq_ref (list): List of reference sequences.\n",
        "            - seq_alt (list): List of alternative sequences.\n",
        "            - seq_val (list): List of values.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    seq_ref = []\n",
        "    seq_alt = []\n",
        "    \n",
        "\n",
        "    # Iterate over the DataFrame rows\n",
        "    for idx, row in data_df.iterrows():\n",
        "        chromosome = f\"{row['Chromosome']}\"\n",
        "        abspos = row['Position']\n",
        "        \n",
        "        # Calculate the start and end positions for the sequence extraction\n",
        "        start_pos = abspos - (length_bp // 2)-1\n",
        "        end_pos = abspos + (length_bp // 2)  # Add 1 to ensure the length is exactly 1000 bp\n",
        "        \n",
        "        # Extract the sequence from the chromosome data\n",
        "        seq = str(chrom2seq[chromosome][start_pos:end_pos])\n",
        "        if len(seq) != length_bp:\n",
        "            raise ValueError(f\"Extracted sequence length {len(seq)} does not match the expected length {length_bp}.\")\n",
        "        \n",
        "        half_len = len(seq) // 2\n",
        "\n",
        "        #seq_ref.append(seq)\n",
        "        seq_ref.append(f\"{seq[:half_len]}{row['Reference']}{seq[half_len + 1:]}\")\n",
        "        \n",
        "\n",
        "        # Create the alternative sequence by replacing the middle base with 'Alt'\n",
        "        seq_alt.append(f\"{seq[:half_len]}{row['Alternative']}{seq[half_len + 1:]}\")\n",
        "\n",
        "        if seq[half_len]!= row['Reference'] and seq[half_len]!= row['Alternative']:\n",
        "            print(\"Warning Nucleaotide does NOT matched Ref or Alt\")\n",
        "\n",
        "    data_df['Seq_Reference'] = seq_ref\n",
        "    data_df['Seq_Alternative'] = seq_alt\n",
        "    return data_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get model predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Function to get predictions in batches\n",
        "def get_predictions_raw(models_tokenizers_dict, seq_ref, seq_alt, device=\"cuda\", batch_size=32):\n",
        "    models_predictions = {}\n",
        "\n",
        "    def tokenize_in_batches(sequence, tokenizer, max_length=512, batch_size=32):\n",
        "        tokens = tokenizer(sequence, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length)\n",
        "        for i in range(0, tokens['input_ids'].size(0), batch_size):\n",
        "            yield {k: v[i:i+batch_size].to(device) for k, v in tokens.items()}\n",
        "\n",
        "    for model_name, item in models_tokenizers_dict.items():\n",
        "        model = item['model'].to(device).eval()\n",
        "        tokenizer = item['tokenizer']\n",
        "\n",
        "        print(f\"Processing model: {model_name}\")\n",
        "\n",
        "        outputs_ref = []\n",
        "        outputs_alt = []\n",
        "\n",
        "        # Process in batches\n",
        "        for inputs_ref in tokenize_in_batches(seq_ref, tokenizer, batch_size=batch_size):\n",
        "            with torch.no_grad():\n",
        "                batch_outputs_ref = model(**inputs_ref).logits.cpu()\n",
        "                outputs_ref.append(batch_outputs_ref)\n",
        "            torch.cuda.empty_cache()  # Clear memory after each batch\n",
        "\n",
        "        for inputs_alt in tokenize_in_batches(seq_alt, tokenizer, batch_size=batch_size):\n",
        "            with torch.no_grad():\n",
        "                batch_outputs_alt = model(**inputs_alt).logits.cpu()\n",
        "                outputs_alt.append(batch_outputs_alt)\n",
        "            torch.cuda.empty_cache()  # Clear memory after each batch\n",
        "\n",
        "        # Concatenate all batch results\n",
        "        outputs_ref = torch.cat(outputs_ref, dim=0)\n",
        "        outputs_alt = torch.cat(outputs_alt, dim=0)\n",
        "\n",
        "        # Store results in CPU memory\n",
        "        models_predictions[model_name] = {'ref': outputs_ref, 'alt': outputs_alt}\n",
        "\n",
        "        # Free GPU memory by moving model to CPU and clearing cache\n",
        "        model.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return models_predictions\n",
        "\n",
        "# Usage\n",
        "#models_predictions = get_predictions_raw(models_tokenizers_dict, data_df['Seq_Reference'].to_list(), data_df['Seq_Alternative'].to_list() , batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_delta(outputs_ref_cpu, outputs_alt_cpu, seq_val):\n",
        "    delta_ref = outputs_ref_cpu[:,1] - outputs_ref_cpu[:,0]\n",
        "    delta_alt = outputs_alt_cpu[:,1] - outputs_alt_cpu[:,0]\n",
        "\n",
        "    # Calculate the difference in logits between alternative and reference sequences\n",
        "    log2_fold_change =  np.log2(torch.sigmoid(delta_alt)/torch.sigmoid(delta_ref))\n",
        "    diff_alt_ref =  np.array(delta_alt)-np.array(delta_ref)\n",
        "\n",
        "    # Compute the difference in the logit values for the positive class (enhancer)     \n",
        "    log2_variant_expression_effect = np.log2(seq_val) \n",
        "            \n",
        "    return np.array(log2_fold_change), log2_variant_expression_effect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_regression_and_correlation(deltas):\n",
        "    slope, intercept, r_val, p_val, std_err = stats.linregress(deltas)\n",
        "    spearman_corr = stats.spearmanr(deltas[0], deltas[1]).correlation\n",
        "    return slope, intercept, r_val, p_val, std_err, spearman_corr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Parameters (load models for prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "# model name from huggingface.co/model name_id:model_name\n",
        "models_names = [\n",
        " 'dnabert2',\n",
        " 'nucleotide-transformer-v2-50m-multi-species',\n",
        " 'nucleotide-transformer-v2-100m-multi-species',\n",
        " 'nucleotide-transformer-v2-250m-multi-species',\n",
        " 'nucleotide-transformer-v2-500m-multi-species',\n",
        " 'nucleotide-transformer-500m-1000g',\n",
        " 'nucleotide-transformer-500m-human-ref',\n",
        " 'nucleotide-transformer-2.5b-1000g',\n",
        " 'nucleotide-transformer-2.5b-multi-species',\n",
        " 'Geneformer',\n",
        " 'gena-lm-bert-base-t2t',\n",
        " 'gena-lm-bert-large-t2t',\n",
        " 'gena-lm-bert-base-t2t-multi',\n",
        " 'gena-lm-bigbird-base-t2t',\n",
        " 'hyenadna-small-32k-seqlen-hf',\n",
        " 'hyenadna-medium-160k-seqlen-hf',\n",
        " 'hyenadna-medium-450k-seqlen-hf',\n",
        " 'hyenadna-large-1m-seqlen-hf'\n",
        " ]\n",
        "\n",
        "# type of fine-tuned\n",
        "ft_model_type = '1kbpHG19_DHSs_H3K27AC'\n",
        "\n",
        "# samples for fine-tuning\n",
        "#'BioS2'=Hela, 'BioS45'=neural progenitor cell, 'BioS73'=hepg2, 'BioS74'=k562\n",
        "bios_ids = ['BioS2', 'BioS45', 'BioS73', 'BioS74']\n",
        "\n",
        "FASTA_FILE_19 = \"/data/Dcode/gaetano/repos/fasta_files/hg19.fa\"\n",
        "FASTA_FILE_38 = \"/data/Dcode/gaetano/repos/fasta_files/hg38.fa\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_to_source = pd.read_csv('/data/Dcode/gaetano/repos/AI4Genomic/data/data_mutagenesis.csv')\n",
        "data_to_source['type_data'] = data_to_source['path'].apply(lambda x: x.split('/')[-2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BioS73, hepg2, hg19, raQTL, hepg2.sign.id.LP190708.txt\n",
            "Loading model and tokenizer for: tanoManzo/dnabert2_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Loading model and tokenizer for: tanoManzo/nucleotide-transformer-v2-50m-multi-species_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Loading model and tokenizer for: tanoManzo/nucleotide-transformer-v2-100m-multi-species_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Loading model and tokenizer for: tanoManzo/nucleotide-transformer-v2-250m-multi-species_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Loading model and tokenizer for: tanoManzo/nucleotide-transformer-v2-500m-multi-species_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Loading model and tokenizer for: tanoManzo/nucleotide-transformer-500m-1000g_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Loading model and tokenizer for: tanoManzo/nucleotide-transformer-500m-human-ref_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Loading model and tokenizer for: tanoManzo/nucleotide-transformer-2.5b-1000g_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "418d084c2bb944829a344fd7f7afb728",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model and tokenizer for: tanoManzo/nucleotide-transformer-2.5b-multi-species_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f42efc446e4942d48d5efd0e201671d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model and tokenizer for: tanoManzo/Geneformer_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Loading model and tokenizer for: tanoManzo/gena-lm-bert-base-t2t_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForMaskedLM were not initialized from the model checkpoint at tanoManzo/gena-lm-bert-base-t2t_ft_BioS73_1kbpHG19_DHSs_H3K27AC and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model and tokenizer for: tanoManzo/gena-lm-bert-large-t2t_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForMaskedLM were not initialized from the model checkpoint at tanoManzo/gena-lm-bert-large-t2t_ft_BioS73_1kbpHG19_DHSs_H3K27AC and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model and tokenizer for: tanoManzo/gena-lm-bert-base-t2t-multi_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForMaskedLM were not initialized from the model checkpoint at tanoManzo/gena-lm-bert-base-t2t-multi_ft_BioS73_1kbpHG19_DHSs_H3K27AC and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model and tokenizer for: tanoManzo/gena-lm-bigbird-base-t2t_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Loading model and tokenizer for: tanoManzo/hyenadna-small-32k-seqlen-hf_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Loading model and tokenizer for: tanoManzo/hyenadna-medium-160k-seqlen-hf_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Loading model and tokenizer for: tanoManzo/hyenadna-medium-450k-seqlen-hf_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Loading model and tokenizer for: tanoManzo/hyenadna-large-1m-seqlen-hf_ft_BioS73_1kbpHG19_DHSs_H3K27AC\n",
            "Processing model: dnabert2_ft_BioS73\n",
            "Processing model: nucleotide-transformer-v2-50m-multi-species_ft_BioS73\n",
            "Processing model: nucleotide-transformer-v2-100m-multi-species_ft_BioS73\n",
            "Processing model: nucleotide-transformer-v2-250m-multi-species_ft_BioS73\n",
            "Processing model: nucleotide-transformer-v2-500m-multi-species_ft_BioS73\n",
            "Processing model: nucleotide-transformer-500m-1000g_ft_BioS73\n",
            "Processing model: nucleotide-transformer-500m-human-ref_ft_BioS73\n",
            "Processing model: nucleotide-transformer-2.5b-1000g_ft_BioS73\n",
            "Processing model: nucleotide-transformer-2.5b-multi-species_ft_BioS73\n",
            "Processing model: Geneformer_ft_BioS73\n",
            "Processing model: gena-lm-bert-base-t2t_ft_BioS73\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/data/Dcode/gaetano/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:1141: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing model: gena-lm-bert-large-t2t_ft_BioS73\n",
            "Processing model: gena-lm-bert-base-t2t-multi_ft_BioS73\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Attention type 'block_sparse' is not possible if sequence_length: 174 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3. Changing attention type to 'original_full'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing model: gena-lm-bigbird-base-t2t_ft_BioS73\n",
            "Processing model: hyenadna-small-32k-seqlen-hf_ft_BioS73\n",
            "Processing model: hyenadna-medium-160k-seqlen-hf_ft_BioS73\n",
            "Processing model: hyenadna-medium-450k-seqlen-hf_ft_BioS73\n",
            "Processing model: hyenadna-large-1m-seqlen-hf_ft_BioS73\n"
          ]
        }
      ],
      "source": [
        "idx = 1\n",
        "data_ = data_to_source.iloc[idx]\n",
        "\n",
        "bios_id = data_['BioS']\n",
        "\n",
        "if data_['hg type'] == 'hg19':\n",
        "    chrom2seq = get_chrom2seq(FASTA_FILE_19)\n",
        "else:\n",
        "    chrom2seq = get_chrom2seq(FASTA_FILE_38)\n",
        "    \n",
        "dataset_path = f\"/data/Dcode/gaetano{data_['path']}\"\n",
        "type_data = data_['type_data']\n",
        "name_data = data_['name']\n",
        "\n",
        "print(f'{bios_id}, {data_[\"cell line\"]}, {data_[\"hg type\"]}, {type_data}, {name_data}')\n",
        "\n",
        "models_tokenizers_dict = load_models_and_tokenizers(models_names, bios_id, ft_model_type)\n",
        "\n",
        "data_df = data_preprocessing(type_data, name_data, dataset_path)\n",
        "data_df = process_sequences(data_df, chrom2seq)\n",
        "data_df = data_df.iloc[:100]\n",
        "\n",
        "models_predictions = get_predictions_raw(models_tokenizers_dict, data_df['Seq_Reference'].to_list(), data_df['Seq_Alternative'].to_list() , batch_size=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done - prediction hepg2.sign.id.LP190708.txt saved\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Save dictionary to a pickle file\n",
        "with open(f'/data/Dcode/gaetano/repos/AI4Genomic/data/predictions/{name_data}.pkl', \"wb\") as pickle_file:\n",
        "    pickle.dump(models_predictions, pickle_file)\n",
        "\n",
        "print(f\"Done - prediction {name_data} saved\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
