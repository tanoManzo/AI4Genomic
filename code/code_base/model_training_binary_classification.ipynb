{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETro9oX0g4kH"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://www.github.com/huggingface/transformers\n",
    "#!pip install git+https://github.com/huggingface/accelerate\n",
    "#!pip install bitsandbytes\n",
    "#!pip install einops\n",
    "#!pip install --upgrade torch torchvision\n",
    "#!pip install scikit-learn\n",
    "#!pip install matplotlib\n",
    "#!pip install datasets\n",
    "#!pip install Bio\n",
    "#!pip install pybedtools\n",
    "#!pip install tabulate\n",
    "#!pip install enformer-pytorch\n",
    "#!pip install einops==0.5.0\n",
    "#!pip install git+https://github.com/vchiley/triton.git@triton_pre_mlir_sm90#subdirectory=python\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths & CKPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Datasets\n",
    "# hg19 fasta file\n",
    "FASTA_FILE = \"/data/Dcode/gaetano/repos/fasta_files/hg19.fa\"\n",
    "\n",
    "# training files\n",
    "path_bios = '/data/Dcode/gaetano/repos/AI4Genomic/data/enhancers/biosamples/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I320YzvVg4kF"
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7jNiPHkIg4kI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 13:52:45.595447: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-06 13:52:45.595484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-06 13:52:45.596214: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-06 13:52:45.600818: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-06 13:52:47.347398: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoModel, BertForSequenceClassification\n",
    "from transformers.models.bert.configuration_bert import BertConfig\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset\n",
    "from Bio import SeqIO\n",
    "from pybedtools import BedTool\n",
    "from transformers import EarlyStoppingCallback\n",
    "import importlib\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import random\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gpu():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "clean_gpu()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get fasta hg19 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chrom2seq(capitalize=True):\n",
    "\n",
    "    chrom2seq = {}\n",
    "    for seq in SeqIO.parse(FASTA_FILE, \"fasta\"):\n",
    "        chrom2seq[seq.description.split()[0]] = seq.seq.upper() if capitalize else seq.seq\n",
    "\n",
    "    return chrom2seq\n",
    "\n",
    "chrom2seq = get_chrom2seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'T'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = 159174683        \n",
    "\n",
    "chrom2seq['chr1'][pos-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "8dpH8tOsg4kJ"
   },
   "source": [
    "## Model & Tokenizer & Datasetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture configuration\n",
    "def get_hf_model_tokenizer(model_ckpt):\n",
    "    \n",
    "    if 'dnabert2' in model_ckpt:  # Only for DNABERT models\n",
    "        model_ckpt =  \"vivym/DNABERT-2-117M\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "        model = BertForSequenceClassification.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "\n",
    "\n",
    "    elif 'Geneformer' in model_ckpt:  # Geneformer model\n",
    "        tokenizer = AutoTokenizer.from_pretrained('tanoManzo/Geneformer_ft_Hepg2_1kbpHG19_DHSs_H3K27AC')\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "\n",
    "    elif 'gena-' in model_ckpt:  # Gena models\n",
    "        model = AutoModel.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "        gena_module_name = model.__class__.__module__\n",
    "        \n",
    "        if 'bigbird' in model_ckpt:  # BigBird model under Gena\n",
    "            cls = getattr(importlib.import_module(gena_module_name), 'BigBirdForSequenceClassification')\n",
    "        else:\n",
    "            cls = getattr(importlib.import_module(gena_module_name), 'BertForSequenceClassification')\n",
    "        \n",
    "        model = cls.from_pretrained(model_ckpt, num_labels=2)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "\n",
    "    else:  # Default case for other models\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, trust_remote_code=True)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get HepG2 data - sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bios_sequences(bios_id, path_bios=path_bios, chrom2seq=chrom2seq):\n",
    "    pos_beds = list(BedTool(f'{path_bios}{bios_id}_positive_1kb.bed'))\n",
    "    ctrl_beds = list(BedTool(f'{path_bios}{bios_id}_control_1kb.bed'))\n",
    "\n",
    "    pos_list = []\n",
    "    ctrl_list = []\n",
    "    for chr, start, end  in pos_beds:\n",
    "        pos_list.append(str(chrom2seq[chr][int(start):int(end)]))\n",
    "\n",
    "    for chr, start, end  in ctrl_beds:\n",
    "        ctrl_list.append(str(chrom2seq[chr][int(start):int(end)]))\n",
    "\n",
    "    ctrl_list = random.sample(ctrl_list, len(pos_list))\n",
    "    seq_data = []\n",
    "    seq_data.extend(pos_list)\n",
    "    seq_data.extend(ctrl_list)\n",
    "\n",
    "    labels_data = []\n",
    "    labels_data.extend([1 for _ in range(len(pos_list))])\n",
    "    labels_data.extend([0 for _ in range(len(ctrl_list))])\n",
    "\n",
    "    return seq_data, labels_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe and remove Ns seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clean_sample(seq_data, labels_data, sample_size):\n",
    "    # Create DataFrame\n",
    "    bioS = pd.DataFrame({'seq_data': seq_data, 'labels': labels_data})\n",
    "\n",
    "    # Filter out rows with sequences consisting only of the same character (presumably Ns)\n",
    "    bioS_no_Ns = bioS[bioS['seq_data'].apply(lambda x: len(set(x)) > 1)]\n",
    "\n",
    "    # take a sample based on sample_size\n",
    "    bioS_no_Ns_sampled = bioS_no_Ns.sample(round(len(bioS_no_Ns)*sample_size),random_state=10)\n",
    "    bioS_no_Ns_sampled['labels'].value_counts()\n",
    "    return bioS_no_Ns_sampled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pkplSPeIg4kM"
   },
   "outputs": [],
   "source": [
    "def datasets_split_train_val_test(bioS_no_Ns_sampled):\n",
    "    # Get training data\n",
    "    train_sequences_bioS = bioS_no_Ns_sampled['seq_data'].tolist()\n",
    "    train_labels_bioS = bioS_no_Ns_sampled['labels'].tolist()\n",
    "\n",
    "    # Split the dataset into a training and a validation dataset\n",
    "    train_sequences_bioS, test_sequences_bioS, train_labels_bioS, test_labels_bioS = train_test_split(train_sequences_bioS,\n",
    "                                                                                  train_labels_bioS, test_size=0.20, random_state=42)\n",
    "\n",
    "    # Split the test data into validation and test sets\n",
    "    validation_sequences_bioS, test_sequences_bioS, validation_labels_bioS, test_labels_bioS = train_test_split(test_sequences_bioS, test_labels_bioS, test_size=0.50, random_state=42)\n",
    "\n",
    "    # Create datasets from dictionaries\n",
    "    ds_train_bioS = Dataset.from_dict({\"data\": train_sequences_bioS, \"labels\": train_labels_bioS})\n",
    "    ds_validation_bioS = Dataset.from_dict({\"data\": validation_sequences_bioS, \"labels\": validation_labels_bioS})\n",
    "    ds_test_bioS = Dataset.from_dict({\"data\": test_sequences_bioS, \"labels\": test_labels_bioS})\n",
    "    \n",
    "    return ds_train_bioS, ds_validation_bioS, ds_test_bioS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "u26edP1Kg4kM"
   },
   "source": [
    "### Tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8WHRcViIg4kM"
   },
   "outputs": [],
   "source": [
    "def get_tokenized_dataset(tokenizer, max_length=512):\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        # Encode sequences\n",
    "        encoding = tokenizer(\n",
    "            examples['data'],\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Add labels to the encoding\n",
    "        encoding['labels'] = examples['labels']\n",
    "        \n",
    "        return encoding\n",
    "    \n",
    "    # Tokenize and process the datasets\n",
    "    tokenized_train = ds_train_bioS.map(tokenize_function, batched=True, remove_columns=[\"data\"])\n",
    "    tokenized_validation = ds_validation_bioS.map(tokenize_function, batched=True, remove_columns=[\"data\"])\n",
    "    tokenized_test = ds_test_bioS.map(tokenize_function, batched=True, remove_columns=[\"data\"])\n",
    "\n",
    "    return tokenized_train, tokenized_validation, tokenized_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Arguments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def create_training_args(output_dir, batch_size, learning_rate, num_epochs, log_steps, token):\n",
    "    training_args = TrainingArguments(\n",
    "        hub_model_id= output_dir.split('/')[1],\n",
    "        output_dir=output_dir,  # Directory to save model and logs\n",
    "        per_device_train_batch_size=batch_size,  # Training batch size per device\n",
    "        per_device_eval_batch_size=batch_size,  # Evaluation batch size per device\n",
    "        learning_rate=learning_rate,  # Learning rate\n",
    "        num_train_epochs=num_epochs,  # Number of training epochs\n",
    "        logging_steps=log_steps,  # Logging interval\n",
    "        logging_dir='./logs',  # Directory to store logs\n",
    "        eval_strategy=\"steps\",  # Evaluation strategy\n",
    "        save_strategy=\"steps\",  # Save strategy\n",
    "        save_total_limit=3,  # Maximum number of saved models\n",
    "        disable_tqdm=False,  # Enable tqdm progress bars\n",
    "        load_best_model_at_end=True,  # Load best model at the end of training\n",
    "        metric_for_best_model=\"f1_score\",  # Metric to select the best model\n",
    "        fp16=True,  # Enable mixed precision training\n",
    "        #push_to_hub=True,  # Push model to Hugging Face hub\n",
    "        hub_token=token  # Authentication token for Hugging Face hub\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metric for the evaluation using f1, auc, and prc\n",
    "def compute_metrics_classification_binary(eval_pred):\n",
    "    \"\"\"Computes F1, AUC, PRC, and other metrics for binary classification.\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=-1)\n",
    "    # Get probability predictions for AUC and PRC calculation (assuming it's binary classification)\n",
    "    prob_predictions = eval_pred.predictions[:, 1]  # assuming class 1 is positive\n",
    "    references = eval_pred.label_ids\n",
    "    \n",
    "    r = {\n",
    "        'f1_score': metrics.f1_score(references, predictions),\n",
    "        'precision': metrics.precision_score(references, predictions),\n",
    "        'recall': metrics.recall_score(references, predictions),\n",
    "        'accuracy': metrics.accuracy_score(references, predictions),\n",
    "        'auc': metrics.roc_auc_score(references, prob_predictions),  # AUC score\n",
    "        'prc': metrics.average_precision_score(references, prob_predictions)  # PRC (average precision score)\n",
    "    }\n",
    "    \n",
    "    return r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Trainer class to override the _save method\n",
    "class CustomTrainer(Trainer):\n",
    "    def _save(self, output_dir, state_dict=None):\n",
    "        # Save the model with safe_serialization=False to avoid shared tensor issues\n",
    "        self.model.save_pretrained(output_dir, state_dict=state_dict, safe_serialization=False)\n",
    "        if self.tokenizer is not None:\n",
    "            self.tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HyenaDNAForSequenceClassification were not initialized from the model checkpoint at LongSafari/hyenadna-small-32k-seqlen-hf and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a956a0f35a747c2909dd1399a9ec782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/476 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43cf4be5c6a1458e9f826d9b0a8e09a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee886672db94d868008a8f861ef47b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/60 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Dcode/gaetano/venv/lib/python3.10/site-packages/accelerate/accelerator.py:500: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='501' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 493/1200 00:09 < 00:14, 50.23 it/s, Epoch 8.20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Prc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.331600</td>\n",
       "      <td>0.725684</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.645161</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.752304</td>\n",
       "      <td>0.678095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "The weights trying to be saved contained shared tensors [{'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.5.freq'}, {'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.3.freq'}, {'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.3.freq'}, {'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.3.freq'}] that are mismatching the transformers base configuration. Try saving using `safe_serialization=False` or remove this tensor sharing.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 74\u001b[0m\n\u001b[1;32m     61\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     62\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m     63\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[EarlyStoppingCallback(early_stopping_patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)]\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# push to hub\u001b[39;00m\n\u001b[1;32m     76\u001b[0m trainer\u001b[38;5;241m.\u001b[39mpush_to_hub()\n",
      "File \u001b[0;32m/data/Dcode/gaetano/venv/lib/python3.10/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/Dcode/gaetano/venv/lib/python3.10/site-packages/transformers/trainer.py:2467\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2464\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[1;32m   2465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2467\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2469\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/data/Dcode/gaetano/venv/lib/python3.10/site-packages/transformers/trainer.py:2918\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2915\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   2917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 2918\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2919\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m/data/Dcode/gaetano/venv/lib/python3.10/site-packages/transformers/trainer.py:3008\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   3006\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3007\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[0;32m-> 3008\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_internal_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[1;32m   3011\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[1;32m   3012\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001b[0;32m/data/Dcode/gaetano/venv/lib/python3.10/site-packages/transformers/trainer.py:3623\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[0;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[1;32m   3620\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3625\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[0;32m/data/Dcode/gaetano/venv/lib/python3.10/site-packages/transformers/trainer.py:3727\u001b[0m, in \u001b[0;36mTrainer._save\u001b[0;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[1;32m   3725\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(state_dict, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, WEIGHTS_NAME))\n\u001b[1;32m   3726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3727\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3728\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_safetensors\u001b[49m\n\u001b[1;32m   3729\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n",
      "File \u001b[0;32m/data/Dcode/gaetano/venv/lib/python3.10/site-packages/transformers/modeling_utils.py:2760\u001b[0m, in \u001b[0;36mPreTrainedModel.save_pretrained\u001b[0;34m(self, save_directory, is_main_process, state_dict, save_function, push_to_hub, max_shard_size, safe_serialization, variant, token, save_peft_format, **kwargs)\u001b[0m\n\u001b[1;32m   2757\u001b[0m         error_names\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mset\u001b[39m(shared_names))\n\u001b[1;32m   2759\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_names) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2760\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2761\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe weights trying to be saved contained shared tensors \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m that are mismatching the transformers base configuration. Try saving using `safe_serialization=False` or remove this tensor sharing.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2762\u001b[0m         )\n\u001b[1;32m   2764\u001b[0m \u001b[38;5;66;03m# Shard the model if it is too big.\u001b[39;00m\n\u001b[1;32m   2765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _hf_peft_config_loaded:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The weights trying to be saved contained shared tensors [{'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.3.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.0.mixer.filter_fn.implicit_filter.5.freq'}, {'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.1.mixer.filter_fn.implicit_filter.3.freq'}, {'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.2.mixer.filter_fn.implicit_filter.3.freq'}, {'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.1.freq', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.5.freq', 'hyena.backbone.layers.3.mixer.filter_fn.implicit_filter.3.freq'}] that are mismatching the transformers base configuration. Try saving using `safe_serialization=False` or remove this tensor sharing."
     ]
    }
   ],
   "source": [
    "# Define the working device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# dataset sample size\n",
    "# 0.1 = 10%, 0.2 = 20%, .. , 1.00 = 100\n",
    "sample_size = 1.00\n",
    "\n",
    "### Model \n",
    "# model name from huggingface.co/model name_id:model_name\n",
    "#model_ckpt = 'czl/dnabert2'\n",
    "\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-v2-50m-multi-species'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-v2-100m-multi-species'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-v2-250m-multi-species'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-v2-500m-multi-species'\n",
    "\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-500m-1000g'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-500m-human-ref'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-2.5b-1000g'\n",
    "#model_ckpt = 'InstaDeepAI/nucleotide-transformer-2.5b-multi-species'\n",
    "\n",
    "#model_ckpt ='ctheodoris/Geneformer'\n",
    "\n",
    "#model_ckpt = 'AIRI-Institute/gena-lm-bert-base-t2t'\n",
    "#model_ckpt = 'AIRI-Institute/gena-lm-bert-large-t2t'\n",
    "#model_ckpt = 'AIRI-Institute/gena-lm-bert-base-t2t-multi'\n",
    "#model_ckpt = 'AIRI-Institute/gena-lm-bigbird-base-t2t'\n",
    "\n",
    "model_ckpt = 'LongSafari/hyenadna-small-32k-seqlen-hf'\n",
    "#model_ckpt = 'LongSafari/hyenadna-medium-160k-seqlen-hf'\n",
    "#model_ckpt = 'LongSafari/hyenadna-medium-450k-seqlen-hf'\n",
    "#model_ckpt = 'LongSafari/hyenadna-large-1m-seqlen-hf'\n",
    "\n",
    "\n",
    "# Define configuration parameters\n",
    "BATCH_SIZE = 8\n",
    "LOG_STEPS = 500\n",
    "LEARNING_RATE = 1e-5\n",
    "NUM_EPOCHS = 200\n",
    "TOKEN = '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'\n",
    "\n",
    "\n",
    "# samples for fine-tuning\n",
    "#'BioS2'=Hela, 'BioS45'=neural progenitor cell, 'BioS73'=hepg2, 'BioS74'=k562\n",
    "bios_ids = ['BioS2', 'BioS45', 'BioS73', 'BioS74']\n",
    "\n",
    "\n",
    "\n",
    "for bios_id in bios_ids:\n",
    "    # load model and dataset\n",
    "    model, tokenizer = get_hf_model_tokenizer(model_ckpt=model_ckpt)\n",
    "    seq_data, labels_data = get_bios_sequences(bios_id, path_bios=path_bios, chrom2seq=chrom2seq)\n",
    "    bioS_no_Ns_sampled = get_clean_sample(seq_data=seq_data, labels_data=labels_data, sample_size=sample_size)\n",
    "    ds_train_bioS, ds_validation_bioS, ds_test_bioS = datasets_split_train_val_test(bioS_no_Ns_sampled=bioS_no_Ns_sampled)\n",
    "    ds_tokenized_train, ds_tokenized_validation, ds_tokenized_test = get_tokenized_dataset(tokenizer, max_length=512)\n",
    "    model.config.use_flash_attention = False  \n",
    "    OUTPUT_DIR = f\"ft/{model_ckpt.split('/')[1]}_ft_{bios_id}_1kbpHG19_DHSs_H3K27AC_one_shot\"\n",
    "    training_args = create_training_args(OUTPUT_DIR, BATCH_SIZE, LEARNING_RATE, NUM_EPOCHS, LOG_STEPS, TOKEN)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model.to(device),\n",
    "        args=training_args,\n",
    "        train_dataset=ds_tokenized_train,\n",
    "        eval_dataset=ds_tokenized_validation,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_classification_binary,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "    # push to hub\n",
    "    trainer.push_to_hub()\n",
    "    # clean gpus\n",
    "    clean_gpu()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7211914   0.7060547 ]\n",
      " [ 2.0644531  -1.9433594 ]\n",
      " [-1.4130859   1.3886719 ]\n",
      " ...\n",
      " [-0.9267578   0.88720703]\n",
      " [ 0.17858887 -0.16992188]\n",
      " [-1.2646484   1.2451172 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-0.7211914 ,  0.7060547 ],\n",
       "       [ 2.0644531 , -1.9433594 ],\n",
       "       [-1.4130859 ,  1.3886719 ],\n",
       "       ...,\n",
       "       [-0.9267578 ,  0.88720703],\n",
       "       [ 0.17858887, -0.16992188],\n",
       "       [-1.2646484 ,  1.2451172 ]], dtype=float32), label_ids=array([1, 0, 1, ..., 0, 1, 1]), metrics={'test_loss': 0.475721150636673, 'test_f1_score': 0.8197463768115942, 'test_precision': 0.7494824016563147, 'test_recall': 0.9045477261369316, 'test_accuracy': 0.7907465825446898, 'test_auc': 0.868190003722487, 'test_prc': 0.8528215378984432, 'test_runtime': 5.09, 'test_samples_per_second': 747.35, 'test_steps_per_second': 93.517})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(ds_tokenized_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.7211914   0.7060547 ]\n",
      " [ 2.0644531  -1.9433594 ]\n",
      " [-1.4130859   1.3886719 ]\n",
      " ...\n",
      " [-0.9267578   0.88720703]\n",
      " [ 0.17858887 -0.16992188]\n",
      " [-1.2646484   1.2451172 ]]\n",
      "Metric                        Value\n",
      "-----------------------  ----------\n",
      "test_loss                  0.475721\n",
      "test_f1_score              0.819746\n",
      "test_precision             0.749482\n",
      "test_recall                0.904548\n",
      "test_accuracy              0.790747\n",
      "test_auc                   0.86819\n",
      "test_prc                   0.852822\n",
      "test_runtime               5.0971\n",
      "test_samples_per_second  746.309\n",
      "test_steps_per_second     93.387\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.predict(ds_tokenized_test)\n",
    "\n",
    "\n",
    "# Print evaluation results in a table format\n",
    "print(tabulate(eval_results.metrics.items(), headers=[\"Metric\", \"Value\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(eval_results.predictions, axis=1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
